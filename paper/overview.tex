\section{Overview}
\label{sec:overview}

The literature on incorporating probabilistic techniques into 
program analysis is large and growing, technically deep, and quite
varied.  In this paper we cannot hope to cover all of it, but our
intention is to expose key similarities and differences between 
families of approaches and, in doing so, provide the reader with
intuitions that are often missing in the detailed presentation of
techniques, quite helpful in understanding them.

\subsection{Where do the probabilities come from?}
There are two perspectives adopted in the literature.
Programs are \textit{implicitly} probabilistic because the distributions
from which input values are drawn are not specified in the program,
but are characteristic of the execution environment.
Alternately, programs are \textit{explicitly} probabilistic in 
that the statements
within the program define the input probability distributions.
More generally, a program might combine both implicit and explicit
probabilistic calls.  Method \texttt{m} in Figure~\ref{fig:example}
is an example of such a combined probabilistic program.

It is possible to transform explicit probabilistic constructs,
by introducing auxiliary input variables and then specifying
their distributions.   For the example, this would result
in the addition of two integer input variables 
\begin{quote}
\texttt{m(int x, int b1, int b2) \{...} 
\end{quote}
where the two instances of
\texttt{drawBernoulli(0.5)} expressions would be replaced
by \texttt{b1} and \texttt{b2}, respectively.  The input
distribution for these auxiliary inputs would then be specified
as a set of pairs,
\[
\{ (\lambda x.x<0,0), (\lambda x.x=0,0.5), (\lambda x.x=1,0.5), (\lambda x.x>1,0) \}
\]
where the first component defines the characteristic function
of a set of values and the second component defines the probability
of a value in that set.

In Section~\ref{sec:probspecs} we will see how distributions can
be specified and inferred, from data sets that characterize the
input domain, in a form that 
analyses can consume.  While effective these approaches can be
costly and Section~\ref{sec:computingprobabilities} discusses
techniques for mitigating that cost.

Section~\ref{sec:pdfa} discusses approaches where probabilities
governing specific branches outcomes, as opposed to input values,
are built into the program model from knowledge the developer
has at hand, while Sections~\ref{sec:computingprobabilities}
and \ref{sec:pse} compute such probabilities from information about
the program semantics and the input distribution.

\subsection{What does the analysis compute?}
There are again two perspectives adopted in the literature.
One can view a probabilistic program as a transformer on probability
distributions and compute the probability distribution, over the
concrete domain, that holds at a program state.
Alternatively, one can view a probabilistic program as a program 
whose inputs happen
to vary in some principled way and compute program properties, 
properties of sets of concrete domain elements, along with a characterization
of how that property varies with varying input.
Within these approaches there are types of approximations
computed for probabilities.  It is common to compute upper bounds
on probabilities for program properties, but lower bounds can 
be computed as well.  In addition, it is possible to estimate the
probability within some margin of error -- an approach that several
techniques explore -- and it is even possible to compute the probability
\textit{exactly} if certain restrictions hold on the program and distributions.

Conceptually, there are two pieces of information that are necessary
to reason probabilisticaly about a set of concrete values: a quantity
that approximates the probability of each value and the number of
values in the set.  Many of the earlier probabilistic static analysis
techniques did not explicitly capture this latter quantity, but
more recent work discussed in Section~\ref{pse}, using
techniques discussed in Section~\ref{sec:computingprobabilities}, 
as well as other recent approaches~\cite{probablisticPolyhedra} do.

\ignore{
example showing a negation function with drawBernoulli(0.25)
and how it transforms a distribution

show a program that tests whether the absolute value of a number
is greater than 5 where the input is distributed according to N(0,1)
}


\subsection{Mixing abstraction with probabilities}
Any analysis that hopes to scale will have to symbolically approximate
behavior.  As explained earlier, in static analyses it is common
to model such overapproximation using non-deterministic choice.
Across all of the analysis techniques we have surveyed, MDPs
have been used when there is a need to mix probabilistic
and non-deterministic choice.   
An important consequence of using MDPs is that it is no longer possible
to compute a single probabilistic characterization of a property.
Instead analyses can compute, across the set of all possible sequences
of non-deterministic choice outcomes, the minimal and maximal 
probabilities for a property to hold.

If the minimal probability for a property of interest lies above 
a desired probability threshold, then regardless of how the non-determinism
is resolved the property is guaranteed to hold with the desired 
threshold -- the probabilistic property \textit{must} hold.  
If that is not the case, but the maximal probability for
a property of interest lies above a desired threshold, then there
there is a schedule to resolve the non-determinism that satisfies
the property with at least the desired property -- the probabilistic property \textit{may} hold.

