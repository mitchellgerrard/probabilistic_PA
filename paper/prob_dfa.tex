\section{Probabilistic Data Flow Analysis}
\label{sec:pdfa}

As we will see in all sections, probabilistic data flow
analysis moves from the true/false nature of its classical
counterpart to the probably-true/probably-false nature of
a static analysis extended with probabilities.
The shift from the qualitative to the quantitative allows
you to incorporate probabilistic information into the
analysis in different ways.

\subsection{Control Probabilities}

Initial work in extending data flow analysis 
techniques with probabilities did not consider
the semantics of the program; instead, already-given probabilities 
were attached to nodes in that program's control flow graph.  
The goal was to predict the probability of an expression evaluating 
to some value or type at runtime, which could 
allow you to perform useful program optimizations.

This approach begins with a control flow graph where each edge $e$ is 
mapped to the probability that $e$ is taken during execution.
The sum of all probabilities leaving any control flow node must be 1
(excepting the exit node).
These probabilities may be obtained through heuristics, profiling,
or some static analysis.
Imagine an execution trace following some path along the edges of
the control flow graph.
The probability of executing that trace is expressed as the product of 
edge probabilities along this path.
So the probability of executing some program point can be seen as the
summation of the probabilities of traces which can reach that program
point.

\mycomment{Mitch: insert figure to make explanation more concise}

Within this bag of execution traces which reach point $u$, we want to 
find the portion of traces which satisfy some data flow fact $d$.  
The ratio of the satisfying portion to the size of the trace bag
gives us the probability of fact $d$ holding at point $u$.

To compute this expected frequency, Ramalingam uses a slightly
modified version of Kildall's dataflow analysis framework.
Instead of the usual semilattice with an idempotent meet
operation, we use the non-idempotent addition operator.
The restricted properties of the meet operation can be
relaxed because instead of computing invariant dataflow
fact, we only want the summation of probabilities of all
traces reaching a certain point.
The expected frequencies may now be computed as the least
fixed point using the same iterative algorithm presented
in the background; the quantity becomes a
{\sl sum-over-all-paths} instead of a {\sl meet-over-all-paths}.

\mycomment{Mitch: Ramalingam also deals with a slight variation
on a lattice, but his variation is close enough to a traditional
lattice that I don't think we need to mention this?}

Ramalingam's work assumes execution history does not matter 
(the analysis is path insensitive).
Later work (Mehofer) adds some path sensitivity, but as both
frameworks deals with exploded control flow graphs, a fully 
path-sensitive approach is not tractable.

\subsection{Data Probabilities}

Within the last 15 years, research in probabilistic data flow analysis
began incorporating probabilistic information directly into
the semantics of a program.
This is typically done using a variation on Kozen's 
probabilistic semantics alongside traditional data
flow techniques.
Embedding probabilities into the semantics allows probabilistic
information to influence how both control {\sl and} data structure
probabilities are computed during the analysis.

The remaining approaches in this section permit probabilistic
information to be defined as either an environment property
(i.e., distribution given for an input) or in the type of
an expression (i.e., rand call).
We examine techniques that use the framework of abstract
interpretation.

How do the classical abstract domains work in a
probabilistic setting?
We will focus on one technique developed by Monniaux
that requires little change to the classical domain.
The key difference between the classical and the probabilistic 
case is that in the probabilistic case, 
an abstract domain has a weight attached to any of its subsets.
More formally
$\mathcal{A}_p \equiv \mathcal{P}(\mathcal{A} \times \mathcal{R}^+)$, 
i.e., a set of pairs $(a,w)$ where $a \in \mathcal{A}$ implicitly
defines a set of concrete values, $\gamma(a)$, such that
$\forall c \in \gamma(a) : Pr(c) \le w$.

The valuation of any element in the concrete domain then 
becomes the additive composition of weights of points in 
the concrete domain which are represented by elements in
the abstract domain.
For instance if $ap \in \mathcal{A}_p$ is such that
$(a_i,w_i) \in ap$ and 
$(a_j,w_j) \in ap$ then $c \in \gamma(a_i) \wedge
c \in \gamma(a_j) \implies Pr(c) \le w_i + w_j$.
The concretization function maps from a set of abstract
values to a weight function.

We cannot go over the details of probabilistic semantics here, but there
are a few modifications to the abstract semantics of a
traditional imperitive program (see Hankin's WHILE language) 
which we will point out.
One is the addition of a random number generator primitive; it
is possible and straightforward to approximate a safe upper
bound on this generator.
Approximations on loop semantics are dealt with in a safe way
using ``suitable" widening operators instead of fixed points.

Conditionals can be seen as filtering
the abstract domain between those execution environments which
satisfy the conditional and those which falsify the conditional. 
The difference in the probabilistic abstract environment with weights 
is that the filter is only applied to the first component of
the tuple (the traditional elements of an abstract domain), 
and leaves the weight unchanged.
For instance, consider the abstract domain of an interval of 
integers defined by the tuple, $([-5,5],0.1)$. 
If this domain holds before a conditional of 
{\tt if(x<0)\{...\}}, after applying the filter on the true branch, 
we get $([-5,-1],0.1)$. 
After applying the filter on the false branch, we get $([0,5],0.1)$.
The space is reduced; the weights remain the same.

An example will be helpful (simplified from Monniaux's (2004)).
We will work with the abstract domain of intervals in $\mathcal{R}$,
Consider the following C-like program, where {\tt centered\_uniform()}
and {\tt abs(x)} return a {\tt double} uniformly distributed in 
$[-1,1]$ and the absolute value of {\tt x}, respectively.

{\tt double x;\\
     .../* A */\\
     x = centered\_uniform();\\
     if(abs(x) <= 1) \{ /* B */ \}}

We want to find a weight function which, for any element in the
concrete domain, sums up the weights of the corresponding elements
in the abstract domain.

\mycomment{Mitch : should our example include overlapping intervals
to highlight the ``summation" aspect of the weight function?}

This correctness criterion is given as an upper bound on the
probability of some outcome in the program, e.g. {\sl the
probability of violation $\phi$ is less than $0.0001\%$}.
Dually, other approaches have
used lower probability bounds as their correctness criterion.

\subsection{Modeling Nondeterminism}

There are variations of uncertainty,
and not every program property should be modeled by a
probability distribution.
For instance, a user may exploit an unlikely control sequence
in a vending machine to get free candy bars. 
If word gets out, the probability of
this exploited behavior occurring is poorly modeled by a uniform 
random distribution.
It is better to treat this kind of input nondeterminisitically.

In Monniaux's semantics, choices that can be tied to a known
distribution are cleanly separated from those that cannot.
A nondeterministic choice allows for independent outcomes, and
this is modeled by lifting the singleton outcomes of deterministic
semantics to powersets of outcomes.
In the probabilistic setting, the elements of this powerset are
tuples of the abstract domain and the associated weight, defined
above.
So for any nondeterminstic choice, the resulting computation 
is safely modeled by one of these tuples.

\subsection{More and varied probabilistic data flow analyses}

There has been recent work which explores computing {\sl expectation
invariants}.
Instead of giving safe bounds on an over-approximation of a set
of states, this approach computes the long-run expected value of
some property, e.g. 
{\sl this variable will be an even integer at this program point
$67\%$ of the time}.
Chakarov et al. compute this using an iterative analyis within
abstract interpretation.
Using a different approach, Di Pierro et al. define the abstract 
domain over vector spaces, instead of lattices.
The dataflow information is collected using the Moore-Penrose
pseudo-inverse instead of the usual fixed point.
The matrices quickly become large; it is not clear how this
approach scales with bigger programs.

The idea of a ``probabilistic program" has been generalized from
Kozen's original semantics to include conditioning on program
observations.
In this setting, the program implicitly specifies a probability 
distribution conditioned on these stated observations.
The distribution is discovered by running (sampling) the program many
times; if some program trace does not match an observation
at a given point, that trace is not considered, and a new ``sample"
of the program is run. 
Data flow analyses have recently been adapted to perform Bayesian
inference on this new class of probabilistic programs.

\ignore{
\subsection{Meta-comments}

We have chosen to organize the work on prob. data flow analysis based 
on how the probabilistic information is incorporated into the analysis
(e.g., probabilities on data, probabilities on control)
and on the nature of approximation in the analysis, 
i.e., underapproximation, overapproximation, or "tight" approximation.

We plan a separate discussion of how non-deterministic choice is
handled in data flow analysis.

Finally, we plan a brief mention of work that does not fit into 
"basic probabilistic program" category, i.e., programs that use
conditioning.

We would be interested in exposing other dimensions ASAP.  Specifically,
are there different dimensions that might arise due to thinking about 
model checking or symbolic execution?

\subsection{Required Terminology}

The following terms/concepts should be defined earlier in the paper
since we will need them in this section.

\begin{enumerate}

 \item probabilistic program
 \item concrete domain
 \item abstract domain
 \item fixpoint
 \item abstract interpretation/data flow analysis
 \item program trace
 \item path
 \item conditioned distribution
 \item Bayesian inference

\end{enumerate}

We expect that this will be done in the intro and background section.   With
regards to that section it would be ideal if we could have a compact
explanation of non-probabilistic data flow analyis/abstract interpretation,
model checking, and symbolic execution with the attendant concepts.
That will cover most of the above and then we can have a separate
subsection of the background covering the probabilistic 
concepts/terms/definitions.

\subsection{The Outline}

Probabilistic Data Flow Analysis Outline

We are considering approaches that start from classical abstract domains.
and characterize the probability of properties expressed as subsets
of those domains holding at program points.
  - this is equivalent to reasoning about the probability of assertions
    holding or not (in prob sym exe) or probabilistic universal properties
    (in prob model checking) 

\begin{enumerate}

 \item Probabilities on control structure
   \begin{enumerate}
    \item probabilistic information explicitly annotates the control flow
    structure of the program
    \item Frequency analysis
      \begin{enumerate}
       \item Ramalingam
       \item See if we can tie this approach to the linear
  	  operators representing transfer functions
  	  which both Monniaux and Di Pierro use
      \end{enumerate}
   \end{enumerate}

 \item Probabilities on data structure
   \begin{enumerate}
    \item probabilistic information annotates the data structure of the
    program and its influence on control and data is computed through
    the analysis
    \item the rest of the approaches permit probabilistic information to
    be defined as either an environment property (i.e., distribution
    given for an input) or in the type of an expression (i.e., rand call)
   \end{enumerate}

 \item Bounding property probabilities
   \begin{enumerate}
    \item Monniaux
      \begin{enumerate}
       \item From above
       \item From below
      \end{enumerate}
   \end{enumerate}

 \item Estimating property probabilities
   \begin{enumerate}
    \item PAI
     \begin{enumerate}
      \item Di Pierro
      \item "Tight" in a least-square sense
     \end{enumerate}
   \end{enumerate}

 \item Treating uncertainty expressed as non-determinism
   \begin{enumerate}
    \item Monniaux can do this 
   \end{enumerate}

 \item Expanding the scope of analysis
   \begin{enumerate}
    \item ... to different probabilistic properties
      \begin{enumerate}
        \item Chakarov
        \item Fixed Points
        \item Martingales
      \end{enumerate}
    \item ... to more general probabilistic programs
      \begin{enumerate}
        \item Bayesian inference
          \begin{enumerate}
            \item Nori
            \item Modern Probabilistic Programming Languages
          \end{enumerate}
      \end{enumerate}
   \end{enumerate}

\end{enumerate}
}
