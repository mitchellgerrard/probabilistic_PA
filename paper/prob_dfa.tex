\section{Probabilistic Data Flow Analysis}
\label{sec:pdfa}

As we will see in all sections, probabilistic data flow
analysis moves from the true/false nature of its classical
counterpart to the probably-true/probably-false nature of
a static analysis extended with probabilities.
The shift from the qualitative to the quantitative allows
you to incorporate probabilistic information into the
analysis in different ways.

\subsection{Control Probabilities}

Initial work in extending data flow analysis 
techniques with probabilities did not consider
the semantics of the program; instead, already-given probabilities 
were attached to nodes in that program's control flow graph.  
The goal was to predict the probability of an expression evaluating 
to some value or type at runtime, which could 
allow you to perform useful program optimizations.

This approach begins with a control flow graph where each edge $e$ is 
mapped to the probability that $e$ is taken during execution.
The sum of all probabilities leaving any control flow node must be 1
(excepting the exit node).
These probabilities may be obtained through heuristics, profiling,
or some static analysis.
Imagine an execution trace following some path along the edges of
the control flow graph.
The probability of executing that trace is expressed as the product of 
edge probabilities along this path.
So the probability of executing some program point can be seen as the
summation of the probabilities of traces which can reach that program
point.

\mycomment{Mitch: insert figure to make explanation more concise}

Within this bag of execution traces which reach point $u$, we want to 
find the portion of traces which satisfy some data flow fact $d$.  
The ratio of the satisfying portion to the size of the trace bag
gives us the probability of fact $d$ holding at point $u$.

To compute this expected frequency, Ramalingam uses a slightly
modified version of Kildall's dataflow analysis framework
~\cite{ramalingam1996data}.
Instead of the usual semilattice with an idempotent meet
operation, we use the non-idempotent addition operator.
The restricted properties of the meet operation can be
relaxed because instead of computing invariant dataflow
fact, we only want the summation of probabilities of all
traces reaching a certain point.
The expected frequencies may now be computed as the least
fixed point using the same iterative algorithm presented
in the background; the quantity becomes a
{\sl sum-over-all-paths} instead of a {\sl meet-over-all-paths}.

\mycomment{Mitch: Ramalingam also deals with a slight variation
on a lattice, but his variation is close enough to a traditional
lattice that I don't think we need to mention this?}

Ramalingam's work assumes execution history does not matter 
(the analysis is path insensitive).
Later work (Mehofer) adds some path sensitivity \cite{mehofer2001novel}, 
but as both frameworks deals with exploded control flow graphs, a fully 
path-sensitive approach is not tractable.

\subsection{Data Probabilities}

Within the last 15 years, research in probabilistic data flow analysis
began incorporating probabilistic information directly into
the semantics of a program.
This is typically done using a variation on Kozen's 
probabilistic semantics alongside traditional data
flow techniques.
Embedding probabilities into the semantics allows probabilistic
information to influence how both control {\sl and} data structure
probabilities are computed during the analysis.

The remaining approaches in this section permit probabilistic
information to be defined as either an environment property
(i.e., distribution given for an input) or in the type of
an expression (i.e., rand call).
We examine techniques that use the framework of abstract
interpretation.

How do the classical abstract domains work in a
probabilistic setting?
We will focus on one technique developed by Monniaux
that requires little change to the classical domain 
\cite{monniaux2001backwards}.
The key difference between the classical and the probabilistic 
case is that in the probabilistic case, 
an abstract domain has a weight attached to any of its subsets.
More formally
$\mathcal{A}_p \equiv \mathcal{P}(\mathcal{A} \times \mathcal{R}^+)$, 
i.e., a set of pairs $(a,w)$ where $a \in \mathcal{A}$ implicitly
defines a set of concrete values, $\gamma(a)$, such that
$\forall c \in \gamma(a) : Pr(c) \le w$.

The valuation of any element in the concrete domain then 
becomes the additive composition of weights of points in 
the concrete domain which are represented by elements in
the abstract domain.
For instance if $ap \in \mathcal{A}_p$ is such that
$(a_i,w_i) \in ap$ and 
$(a_j,w_j) \in ap$ then $c \in \gamma(a_i) \wedge
c \in \gamma(a_j) \implies Pr(c) \le w_i + w_j$.
The concretization function maps from a set of abstract
values to a weight function.

We cannot go over the details of probabilistic semantics here, but there
are a few modifications to the abstract semantics of a
traditional imperitive program (see Hankin's WHILE language) 
which we will point out.
One is the addition of a random number generator primitive; it
is possible and straightforward to approximate a safe upper
bound on this generator.
Approximations on loop semantics are dealt with in a safe way
using ``suitable" widening operators instead of fixed points.

Conditionals can be seen as filtering
the abstract domain between those execution environments which
satisfy the conditional and those which falsify the conditional. 
The difference in the probabilistic abstract environment with weights 
is that the filter is only applied to the first component of
the tuple (the traditional elements of an abstract domain), 
and leaves the weight unchanged.
For instance, consider the abstract domain of an interval of 
integers defined by the tuple, $([-5,5],0.1)$. 
If this domain holds before a conditional of 
{\tt if(x<0)\{...\}}, after applying the filter on the true branch, 
we get $([-5,-1],0.1)$. 
After applying the filter on the false branch, we get $([0,5],0.1)$.
The space is reduced; the weights remain the same.

An example will be helpful (simplified from Monniaux's (2004)).
We will work with the abstract domain of intervals in $\mathcal{R}$,
Consider the following C-like program, where {\tt centered\_uniform()}
and {\tt abs(x)} return a {\tt double} uniformly distributed in 
$[-1,1]$ and the absolute value of {\tt x}, respectively.

{\tt double x;\\
     .../* A */\\
     x = centered\_uniform();\\
     if(abs(x) <= 1) \{ /* B */ \}}

We want to find a weight function which, for any element in the
concrete domain, sums up the weights of the corresponding elements
in the abstract domain.

\mycomment{Mitch : should our example include overlapping intervals
to highlight the ``summation" aspect of the weight function?}

This correctness criterion is given as an upper bound on the
probability of some outcome in the program, e.g. {\sl the
probability of violation $\phi$ is less than $0.0001\%$}.
Dually, other approaches have
used lower probability bounds as their correctness criterion.

\subsection{Handling nondeterminism}
\mycomment{Matt: I am working in this section - reading on last paper before
I edit}

There are variations of uncertainty,
and not every program property should be modeled by a
probability distribution.
For instance, a user may exploit an unlikely control sequence
in a vending machine to get free candy bars. 
If word gets out, the probability of
this exploited behavior occurring is poorly modeled by a uniform 
random distribution.
It is better to treat this kind of input nondeterminisitically.

In Monniaux's semantics, choices that can be tied to a known
distribution are cleanly separated from those that cannot.
A nondeterministic choice allows for independent outcomes, and
this is modeled by lifting the singleton outcomes of deterministic
semantics to powersets of outcomes.
In the probabilistic setting, the elements of this powerset are
tuples of the abstract domain and the associated weight, defined
above.
So for any nondeterminstic choice, the resulting computation 
is safely modeled by one of these tuples.

\subsection{More and varied probabilistic data flow analyses}

Bounds on the probability of a state property have been well studied.
Di Pierro et al. \cite{di2013probabilistic} aim to estimate the probability of a property,
rather than bound it.  They formulate their an abstract 
domain over vector spaces, instead of lattices, and use
the Moore-Penrose pseudo-inverse instead of the usual fixed point.
While shown to be effective on small programs, the space
complexity of vector space encodings and lack of tool support
have not yet demonstrated the scalability of the approach.
More recent work has explored computing an alternative probabilistic
property called {\sl expectation invariants} \cite{chakarov2014expectation}.
This approach uses an iterative data flow analysis to 
computes a bound on the long-run expected value of
some program expression, e.g. $E[f(uniform(0,10))] < 7$ states that
over a sufficiently large number of runs $f$ when called with
a uniformly distributed number in the range $[0,10]$ will return
a value less than 7.

The idea of a ``probabilistic program" has been generalized from
Kozen's original semantics to include conditioning on program
observations.
In this setting, the program implicitly specifies a probability 
distribution conditioned on these stated observations.
Data flow analyses have recently been adapted to perform Bayesian
inference on this new class of probabilistic 
programs~\cite{claret2013bayesian}.  
