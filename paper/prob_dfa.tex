\section{Probabilistic Data Flow Analysis}
\label{sec:pdfa}

The key challenge in probabilistic data flow analyses is
determining how probabilities are incorporated into the control
and data abstractions that form its fonudation.

\subsection{Control Flow Probabilities}
Early work in extending data flow analysis 
techniques with probabilities did not consider
the semantics of the program. 
Instead given probabilities 
were attached to nodes in the program's control flow graph.  
This allowed the analysis to estimate
the probability of an expression evaluating 
to some value or type at runtime, which could 
allow useful program optimization.

This approach begins with a control flow graph where each edge is 
mapped to the probability that it is taken during execution.
The sum of all probabilities leaving any control flow node must be 1
(excepting the exit node).
These probabilities may be obtained through heuristics, profiling,
or some static analysis.
Imagine an execution trace following some path along the edges of
the control flow graph.
The probability of executing that trace is expressed as the product of 
edge probabilities along this path.
So the probability of executing some program point can be seen as the
summation of the probabilities of traces which can reach that program point.

\input{prob_cfg}
Figure~\ref{fig:prob-cfg} shows the probabilistic CFG for the
example from Figure~\ref{fig:example} given that input $x$
is uniformly distributed in the range $[1,100]$.

To compute the probability of a data flow fact holding 
at a program point, Ramalingam uses a slightly
modified version of Kildall's dataflow analysis framework
~\cite{ramalingam1996data}.
Instead of the usual semilattice with an idempotent meet
operation a non-idempotent addition operator is used.
The restricted properties of the meet operation can be
relaxed because instead of computing invariant dataflow
fact, we only want the summation of probabilities of all
traces reaching a certain point.
The expected frequencies may now be computed as the least
fixed point using the same iterative algorithm presented
in the background; the quantity becomes a
{\sl sum-over-all-paths} instead of a {\sl meet-over-all-paths}.

Ramalingam's work assumes execution history does not matter --  
the analysis is path insensitive.
Later work adds some path sensitivity \cite{mehofer2001novel}, 
but as both frameworks deals with exploded control flow graphs, a fully 
path-sensitive approach is not tractable.

\subsection{Abstract Data Probabilities}
Accurately estimating control flow probabilities is a significant
challenge -- one which does not have an efficient and precise solution.
Within the last 15 years, research in probabilistic data flow analysis
began incorporating probabilistic information directly into
the semantics of a program and then abstracting over 
those semantics \cite{monniaux2000abstract,others}.
This is typically done using a variation on Kozen's 
probabilistic semantics \cite{kozen1981semantics} 
alongside traditional data flow techniques.
Embedding probabilities into the semantics allows probabilistic
information to influence how both control {\sl and} data related
probabilities are computed during the analysis.

The remaining approaches in this section permit probabilistic
information to be defined as either an environment property
(i.e., distribution given for an input) or in the type of
an expression (i.e., rand call).
We examine techniques that use the framework of abstract
interpretation.

How do the classical abstract domains work in a
probabilistic setting?
Monniaux \cite{monniaux2000abstract} developed the key 
insight that most work in this area has built on 
\cite{monniaux2001backwards,wachterVMCAI10,esparzaSAS11}.
The goal is to exploit the rich body of work on developing
abstract domains and associated transformers and extend them
so as to record sub-probability measures for the concrete values
described by domain elements.

A probabilistic abstract domain, $\mathcal{A}_p$, 
is (an indexed) collection of pairs, 
$\mathcal{A} \times [0,1]$,
such that $\forall (a,w) \in \mathcal{A}_p : 
\forall c \in \gamma(a) : Pr(c) \le w$ -- early work only
considered \textit{weights} that define upper bounds on the 
probability of a property. 
The literature has explored such constructions for both
disjoint, i.e., $\forall a,a' \in \mathcal{A} : 
\gamma(a) \cap \gamma(a') = \emptyset$, and non-disjoint abstract domains.

The disjoint case is simpler in that once the analyses computes
the abstracted weight for an abstract domain element there is
no interaction among elements of $\mathcal{A}_p$ during analysis.
In the case of non-disjoint domains and where a properties of interest
does not reside entirely within one element of $\mathcal{A}$ it is
necessary to consider domain element overlap. 
Given a property $p$ an upper approximation of its probability 
at a program location with probabilistic abstract domain
value $pa = \{(a_1,w_1), ..., (a_n,w_n)\}$
is given by $\sum_{j \in \{ i \vert (a_i,w_i) \in pa \wedge   
p \cap \gamma(a_i) \not= \emptyset\}} w_j$.

The design of probabilistic abstract transformers, as with 
classic abstract transformers, can be subtle.
For statements that generate variables drawn from a probability
distribution an upper approximation of the distribution for
regions of the abstract domain is required.  The literature
has constructed these using ad-hoc techniques, but we believe
the methods of Section~\ref{sec:counting} might be applied to 
achieve this and describe this further in Section~\ref{sec:future}.
For sequential statements, weight components are propogated
and abstract domain elements are updated by the underlying transformer.

For conditionals, the transformer can be understood
as filtering the abstract domain between those execution environments which
satisfy the conditional and those which falsify the conditional. 
The difference in the probabilistic abstract environment with weights 
is that the filter is only applied to the first component of
the tuple (the traditional elements of an abstract domain), 
and leaves the weight unchanged.
For instance, consider the abstract domain of an interval of 
integers defined by the tuple, $([-5,5],0.1)$. 
If this domain holds before a conditional of 
{\tt if(x<0)\{...\}}, after applying the filter on the true branch, 
we get $([-5,-1],0.1)$. 
After applying the filter on the false branch, we get $([0,5],0.1)$.
The space is reduced; the weights remain the same.

Finally, reaching fixpoints for rich probabilistic abstract domains
appears to require widening \cite{Monniaux,Esparza:SAS11} to be
cost-effective.

\subsection{Handling nondeterminism}
\mycomment{Matt: more here with discussion of original Monniaux work,
and the PRISM people's approach and VMCAI'10 and SAS'11 papers}

There are variations of uncertainty,
and not every program property should be modeled by a
probability distribution.
For instance, a user may exploit an unlikely control sequence
in a vending machine to get free candy bars. 
If word gets out, the probability of
this exploited behavior occurring is poorly modeled by a uniform 
random distribution.
It is better to treat this kind of input nondeterminisitically.

In Monniaux's semantics, choices that can be tied to a known
distribution are cleanly separated from those that cannot.
A nondeterministic choice allows for independent outcomes, and
this is modeled by lifting the singleton outcomes of deterministic
semantics to powersets of outcomes.
In the probabilistic setting, the elements of this powerset are
tuples of the abstract domain and the associated weight, defined
above.
So for any nondeterminstic choice, the resulting computation 
is safely modeled by one of these tuples.

\subsection{More and varied probabilistic data flow analyses}

Bounds on the probability of a state property have been well studied.
Di Pierro et al. \cite{di2013probabilistic} aim to estimate the probability of a property,
rather than bound it.  They formulate their an abstract 
domain over vector spaces, instead of lattices, and use
the Moore-Penrose pseudo-inverse instead of the usual fixed point.
While shown to be effective on small programs, the space
complexity of vector space encodings and lack of tool support
have not yet demonstrated the scalability of the approach.
More recent work has explored computing an alternative probabilistic
property called {\sl expectation invariants} \cite{chakarov2014expectation}.
This approach uses an iterative data flow analysis to 
compute a bound on the long-run expected value of
some program expression, e.g. $E[f(uniform(0,10))] < 7$ states that,
over a sufficiently large number of runs, when $f$ is called with
a uniformly distributed number in the range $[0,10]$, it will return
an average value less than 7.

The idea of a ``probabilistic program" has been generalized from
Kozen's original semantics to include conditioning on program
observations~\cite{Gordon2014}.
In this setting, the program implicitly specifies a probability 
distribution conditioned on these stated observations.
Data flow analyses have recently been adapted to perform Bayesian
inference on this new class of probabilistic 
programs~\cite{claret2013bayesian}.  
