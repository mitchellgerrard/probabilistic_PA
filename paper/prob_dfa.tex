\section{Probabilistic Data Flow Analysis}
\label{sec:pdfa}

The key challenge in probabilistic data flow analyses is
determining how probabilities are incorporated into the control
and data abstractions that form its foundation.

\subsection{Control Flow Probabilities}
Early work in extending data flow analysis 
techniques with probabilities did not consider
the semantics of the program. 
Instead user-defined probabilities 
were attached to nodes in the program's control flow graph.  
This allowed the analysis to estimate
the probability of an expression evaluating 
to some value or type at runtime, which could 
allow useful program optimization.

This approach begins with a control flow graph where each edge is 
mapped to the probability that it is taken during execution.
The sum of all probabilities leaving any control flow node must be 1
(excepting the exit node).
These probabilities may be obtained through heuristics, profiling,
or some static analysis.
Imagine an execution trace following some path along the edges of
the control flow graph.
The probability of executing that trace is expressed as the product of 
edge probabilities along this path.
So the probability of executing some program point can be seen as the
summation of the probabilities of traces which can reach that program point.

\input{prob_cfg}
Figure~\ref{fig:prob-cfg} shows the probabilistic CFG for the
example from Figure~\ref{fig:example} given that input $x$
is uniformly distributed in the range $[1,100]$.

To compute the probability of a data flow fact holding 
at a program point, Ramalingam uses a slightly
modified version of Kildall's dataflow analysis framework
~\cite{ramalingam1996data}.
Instead of the usual semilattice with an idempotent meet
operation a non-idempotent addition operator is used.
The restricted properties of the meet operation can be
relaxed because instead of computing invariant dataflow
fact, we only want the summation of probabilities of all
traces reaching a certain point.
The expected frequencies may now be computed as the least
fixed point using the same iterative algorithm presented
in the background; the quantity becomes a
{\sl sum-over-all-paths} instead of a {\sl meet-over-all-paths}.

Ramalingam's work assumes execution history does not matter --  
the analysis is path insensitive.
Later work adds some path sensitivity \cite{mehofer2001novel}, 
but as both frameworks deals with exploded control flow graphs, a fully 
path-sensitive approach is not tractable.

Ramalingam's sum-over-all-paths approach is reminiscent of
the approach taken in probabilistic model checking of DTMCs.
In that approach a system of linear equations is formulated
whose solution computes the probability with which a property
holds -- so called \textit{quantitative} properties in 
PRISM \cite{PRISMmarktoberdorf}.   Ramalingam formulates an
equivalent system of linear equations.  

Both of these techniques rely on being able to annotate
branch decisions in the program, or model in PRISM's case, 
with probabilities.  When those decisions are governed by
computed conditions over input variables the calculation of
branch probabilities quickly becomes challenging -- as we will
see in Section~\ref{sec:pse} the techniques from 
Section~\ref{sec:computingprobabilities} can be applied to
this problem.

\ignore{
Accurately estimating control flow probabilities is a significant
challenge, but the techniques from Section~\ref{sec:computingprobabilities}
offer one means of approaching that problem.
}

\subsection{Abstract Data Probabilities}
Within the last 15 years, researchers 
began incorporating probabilistic information directly into
the semantics of a program and then abstracting over 
those semantics \cite{monniaux2000abstract,others}
to enable data flow analysis.
This is typically done using a variation on Kozen's 
probabilistic semantics \cite{kozen1981semantics} 
alongside traditional data flow techniques.
Embedding probabilities into the semantics allows 
the expression of both control and data influences on property
probabilities computed during the analysis.

\subsubsection{Abstracting Probability Distributions}
The first work in this area, by 
Monniaux \cite{monniaux2000abstract,monniaux2001backwards},
developed the key insights that most work in this area has built on. 
The goal is to exploit the rich body of work on developing
abstract domains and associated transformers and extend them
so as to record bounds on probability measures for the concrete values
described by domain elements.

Monniaux's work took the view that probabilistic programs 
effectively transform an input distribution into an output
distribution.  More generally, they compute a distribution that
characterizes each point in the program.   
Here a probabilistic abstract domain, $\mathcal{A}_p$, 
is (an indexed) collection of pairs, $\mathcal{A} \times [0,1]$.
Given a concrete value $c$, 
an upper approximation of its probability 
at a program location with probabilistic abstract domain
value $pa = \{(a_1,w_1), ..., (a_n,w_n)\}$
is given by $\sum_{j \in \{ i \vert (a_i,w_i) \in pa \wedge   
c \in \gamma(a_i)\}} w_j$.   A concrete
value may lie in a number of abstract domains defined by
the probabilistic abstract domain value.  For each such abstract
domain the associated probability \textit{weight} must be totalled
to bound of the probability of the concrete value.

To clarify, these weight components are \textit{not} bounds on the probability
the abstract domain as a whole, but rather they bound the probability
of each concrete element represented by the abstract domain.
This simplifies the formulation of the probabilistic abstract
transformers, i.e., the extension of $\tau^\#$ to account to 
$\mathcal{A}_p$, as discussed below, but it means that additional
work is required to compute the probability of a property holding.
Fundamentally that requires estimating the size of the concretization
of the abstract domain element and then multiplying by the computed bound for
each concrete value.  

Recall that the techniques from 
Section~\ref{sec:computingprobabilities} can be applied to
the problem of counting the concretization of an abstract domain element 
that is encoded as a logical formula.  This may offer a potential
connection between data flow analyses formulated over distributions
and those formulated over abstract states -- which we discuss below.

The design of probabilistic abstract transformers, as with 
classic abstract transformers, can be subtle.
For statements that generate variables drawn from a probability
distribution an upper approximation of the distribution for
regions of the abstract domain is required.  The literature
has constructed these using ad-hoc techniques, but we believe
the methods of Section~\ref{sec:counting} might be applied to 
achieve this and describe this further in Section~\ref{sec:future}.
For sequential statements, weight components are propogated
and abstract domain elements are updated by the underlying transformer.

For conditionals, the transformer can be understood
as filtering the abstract domain between those execution environments which
satisfy the conditional and those which falsify the conditional. 
The difference in the probabilistic abstract environment with weights 
is that the filter is only applied to the first component of
the tuple (the traditional elements of an abstract domain), 
and leaves the weight unchanged.
For instance, consider the abstract domain of an interval of 
integers defined by the tuple, $([-5,5],0.1)$. 
If this domain holds before a conditional of 
{\tt if(x<0)\{...\}}, after applying the filter on the true branch, 
we get $([-5,-1],0.1)$. 
After applying the filter on the false branch, we get $([0,5],0.1)$.
The space is reduced; the weights remain the same.

Finally, reaching fixpoints for rich probabilistic abstract domains
appears to require widening \cite{monniaux2000abstract,esparza2011probabilistic} to be cost-effective.

\subsubsection{Probability for Abstract States}



\mycomment{Matt: discuss Di Pierro's latest work which defines
abstract branch probabilities.}

Bounds on the probability of a state property have been well studied.
Di Pierro et al. \cite{di2013probabilistic} aim to estimate the probability of a property,
rather than bound it.  They formulate their an abstract 
domain over vector spaces, instead of lattices, and use
the Moore-Penrose pseudo-inverse instead of the usual fixed point.
While shown to be effective on small programs, the space
complexity of vector space encodings and lack of tool support
have not yet demonstrated the scalability of the approach.


\subsection{Handling nondeterminism}
When abstraction of program choice are required or when 
there is no basis for defining an input distribution programs,
it is natural to use non-determinism to account for the uncertainty
in program behavior.

In Monniaux's semantics\cite{MonniauxMDP}, choices that can be tied to a known
distribution are cleanly separated from those that cannot.
A nondeterministic choice allows for independent outcomes, and
this is modeled by lifting the singleton outcomes of deterministic
semantics to powersets of outcomes.
In the probabilistic setting, the elements of this powerset are
tuples of the abstract domain and the associated weight, defined
above.  So for any nondeterminstic choice, the resulting computation 
is safely modeled by one of these tuples.  The challenge in the
analysis is to select from among those tuples to compute a useful
probability estimate.

\mycomment{Matt: more here with discussion of original Monniaux work,
add a very short discussion of the 2-player game approach then just
mention these}
\cite{PRISMabstraction,wachter2010best,esparza2011probabilistic}.

\subsection{More and varied probabilistic data flow analyses}
\mycomment{Go through most recent related work and characterize it
here}

More recent work has explored computing an alternative probabilistic
property called {\sl expectation invariants} \cite{chakarov2014expectation}.
This approach uses an iterative data flow analysis to 
compute a bound on the long-run expected value of
some program expression, e.g. $E[f(\mathit{uniform}(0,10))] < 7$ states that,
over a sufficiently large number of runs, when $f$ is called with
a uniformly distributed number in the range $[0,10]$, it will return
an average value less than 7.

The idea of a ``probabilistic program" has been generalized from
Kozen's original semantics to include conditioning on program
observations~\cite{Gordon2014}.
In this setting, the program implicitly specifies a probability 
distribution conditioned on these stated observations.
Data flow analyses have recently been adapted to perform Bayesian
inference on this new class of probabilistic 
programs~\cite{claret2013bayesian}.  
