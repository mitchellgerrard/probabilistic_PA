\subsection{Computing Program Probabilities}
\label{sec:computingprobabilities}

Computing probabilities for probabilistic program analysis can usually be reduced to computing the probability of satisfying a boolean constraint over the program variables. In this section we will introduce some of the basic techniques currently used in program analysis. 

To simplify the notation, we will focus on implicit probabilistic constructs, assuming the program under analysis has input variables $V=\{v_1, v_2, \dots, v_n\}$, where $v_i$ has domain $d_i$ and comes with a probability distribution $\mathcal{P}_i: d_i \to [0, 1]$. The input domain $D$ is defined as the cartesian product of the domains $d_i$, while the input distribution $\mathcal{P}$ is defined as the joint distribution over all the input variables $\prod_i \mathcal{P}_i(\bar{v_i})$. Given a constraint $\phi: V \to \{true, false\}$, our goal is to compute the probability $Pr(\phi)$ of satisfying $\phi$ given the input domains and probability distributions. This problem is usually referred to as model counting, when the input domains are countable, or solution space quantification, when the input domains are modeled as uncountable (e.g., abstracting floating-point numbers as reals).

\subsubsection{Exact and numeric computation}\label{sec:computingprobabilitiesExact}

% OUTLINE
%	\begin{enumerate}
%		\item Finite domains
%			\begin{enumerate}
%				\item Linear integers (Latte, Barvinok, Omega for negation; used in our works)
%				\item Strings (bounds: \url{http://www.comp.nus.edu.sg/~shweta24/publications/smc\_pldi14.pdf} ; automata-based exact and upperbounds: \url{http://www.cs.ucsb.edu/~bultan/publications/model-counting.pdf}; we should also check the related work thereof)
%				\item Data structures (icse13 and spin15)
%				\item Sat and smt (\url{http://arxiv.org/pdf/1306.5726v3.pdf}, \url{http://arxiv.org/pdf/1411.0659.pdf}; these papers should have been published, we should check related work in there)
%			\end{enumerate}
%		
%		\item Uncountable domains
%			\begin{enumerate}
%				\item Symbolic and numerical integration (interesting, but symbolic does not scale apart from a few simple cases, while numeric suffers for large cardinality of input domains; however: general, available off-the-shelf, parallelizable for numerical)
%			\end{enumerate}
%	\end{enumerate}

\paragraph{Finite domains.} 

If the input domain is finite, the classical formulation of probability can be used to reduce the computation of $Pr(\phi)$ to a counting problem (here we assume a uniform distribution over all the possible input values, i.e., each valid input has the same probability):
%
\begin{equation}\label{eq:counting}
	Pr(\phi) = \frac{\sharp(\phi \land D)}{\sharp(D)}
\end{equation}
%
\noindent where $\sharp(\cdot)$ counts the number of inputs satisfying the argument constraint; $D$ has been overloaded to represent the finite domain as a constraint ($\sharp(D)$ is a short form for the size of the domain)\footnote{More precisely, Equation~\eqref{eq:counting} represents the probability of satisfying the constraint $\phi$ conditioned on the fact that the input is within the prescribed domain $D$.}. For example, considering a single integer input variable $x$ taking values between $1$ and $10$ uniformly, $\sharp(D)=10$ and $\sharp(x\leq5 \land D)=5$, leading to a $0.5$ probability of satisfying the constraint.

Efficient implementations of $\sharp(\cdot)$ are available for several classes of model counting problems, we focus here on linear integer arithmetic (LIA).
An LIA constraint---the conjunction of linear constraints over a finite integer domain---geometrically defines a multi-dimensional lattice bounded by a convex polytope~\cite{de2008computationalGeometry}. To count the number of points composing this structure, an efficient solution has been proposed by Barvinok in~\cite{barvinok1994polynomial}. This algorithm is grounded on the construction of generating functions suitable for solving the counting problem in polynomial time, with respect to the number of variables and the number of constraints. Notably, besides the number of bits required to represent the numerical values, the complexity of this algorithm does not depend on the actual size of the variable domains. This makes the computation feasible for very large input domains, allowing its application to probabilistic program analysis. Several implementations of this algorithm are available, the two most popular being LattE~\cite{LattESoftware} and Barvinok~\cite{verdoolaegesoftware}. When disjunctions appear in the constraint, these have to be preprocessed before applying Barvinok's algorithm (e.g., using the Omega library~\cite{Omega1996}). Though this normalization increases the overall complexity of model counting, several optimizations can be leveraged to reduce the computational effort.
%(we will discuss some later in Section~\ref{sec:computingprobabilitiesOptimizations}) 
Barvinok's algorithm has been used for probabilistic program analysis in \cite{Geldenhuys2012,Filieri2013,Filieri2015}.


\ignore{
	\item \textbf{Bounded data structures}: data structures are usually composed of a structural dimension (e.g., lists or trees) and a payload stored in each node of the structure. The level of decoupling between structure and payload differs case by case---for example, a list of integers may be sorted or not. Early work on complexity analysis explored the use of generating functions for representing the number of valid instances of a given data structure without explicitly enumerating them (e.g.,~\cite{flajolet1985mathematical}). However, despite its computational efficiency, this approach requires a significant amount of human work, because the construction of these generating functions can hardly be automatically inferred from the source code. For the sake of generality, early work in probabilistic program analysis~\cite{Filieri2013} used an enumeration-based approach, such as Korat~\cite{Korat2002}. This technique relies on the formalization both of the invariants characterizing the valid structures and of the constraints to be counted as executable boolean methods, and then generates all the instances for which these methods return true. The generation is enhanced with smart pruning techniques to reduce the actual exploration space, although their complexity still leaves most realistic programs out of reach. A recent approach proposed in~\cite{Filieri2015} decouples the structural part from the payload, employing the partial enumeration of the former and the use of more efficient model counting techniques for the latter, whenever possible. For example, in an acyclic sorted list of integers between 1 and 10, having at most 3 elements would require the enumeration of 4 different structural configurations (including the empty list) and the evaluation of 8 linear integer constraints (which can be computed with Barvinok's algorithm), instead of exploring all 1111 possible instances.

	\item \textbf{Regular expressions}: a variety of practically relevant constraints on strings can be formalized as matching with a regular expression~\cite{Luu2014,Aydin2015}. If the (maximum) length of the string is bounded, the number of instances matching a regular expression can be counted exactly with an automata-based approach. The regular expression is first transformed into the corresponding accepting automata. Then, an exponential generating function is automatically constructed to count the accepting runs of the automata up to a certain length, without the need to enumerate all of them explicitly. The technique is actually more general and can be used for any constraint whose satisfaction can be reduced to counting the accepting runs of a finite state automata. For more general constraints, called pseudo-relational, the exact count is not computable, though its value can be bounded in a finite interval, allowing, in some cases, for best- or worst-case analysis~\cite{Aydin2015}.
	
\end{itemize}
}

The counting function $\sharp(\cdot)$ can often be stated as a boolean
satisfiability problem.
The problem of counting the number of distinct truth assignments for
a propositional formula is called \#SAT, or propositional model counting.
There are a number of tools that can efficiently solve many cases of
\#SAT, including sharpSAT~\cite{thurley2006sharpsat} and Cachet~\cite{sang2005heuristics}.

Other finite domains, such as bounded data structures \cite{Filieri2015} and regular languages~\cite{Luu2014,Aydin2015}, are active topics of study in applied model counting.


\paragraph{Floating-point numbers.} 
Floating-point numbers are usually abstracted as real numbers for analysis purposes. Computing the probability of satisfying a constraint over reals requires refining Equation~\ref{eq:counting} to cope with the density of the domain. In particular, the counting function $\sharp(\phi)$ has to be replaced by the integration of an indicator function on $\phi$, i.e., a function returning $1$ for all the inputs satisfying $\phi$ \cite{Borges2014}. This integration can be performed exactly only for a limited number of cases---those where symbolic integration is possible. For all the other cases, only numerical integration is possible. A number of commercial and open-source tools can be used for this purpose, however, 1) numerical computations are accurate only up to a certain bound, and 2) they do not scale when the cardinality of the input domain grows. In the latter case, sampling-based methods are preferable. 

\paragraph{Handling input distributions.} 
For finite domains, we assume, without loss of generality, the input distribution to be specified on a finite partition $D^1, D^2, \dots, D^n$ of the input domain $D$ (i.e., $\cup_i D^i \equiv D$ and $D^i \cap D^j \neq \emptyset \implies i=j$) via the probability function $Pr(D^i)$. We assume elements within the same set $D^i$ to have the same probability. The case of uniform distribution described so far corresponds to the partition with cardinality 1, i.e., the whole domain.
 
Since the elements of the partition are disjoint by construction, we can exploit the law of total probability to extend Equation~\eqref{eq:counting} to include the information about the input distribution:
%
\begin{equation}\label{eq:countingInputDistribution}
	Pr(\phi) = \sum_i \frac{\sharp(\phi \land D^i)}{\sharp(D^i)} \cdot Pr(D^i)
\end{equation}
%
\noindent where $D^i$ has again been overloaded to represent the constraint of an element belonging to $D^i$.

Formalizing the input distribution on a finite partition of the input domain is general enough to capture every valid distribution on the inputs, including possible correlations or functional dependencies among the input variables. However, the finer the specification of the input distribution, the more complex the computation of Equation~\eqref{eq:countingInputDistribution}, which, in the worst case, may require the computation of $|D|$ summands \cite{Borges2014}. While this worst case is unlikely to occur (partially due to the optimization strategies described later), more efficient probability computations are possible which employ distribution-aware sampling-based methods, described in the next section.

\subsubsection{Sampling-based methods}\label{sec:computingprobabilitiesSampling}
Exact methods can suffer from two main limitations: 1) generality with respect to input domains and constraint classes and 2) scalability, either due to the intrinsic complexity of the algorithm used or to the discretization of the input distributions. In many cases, sampling-based methods may be used to leverage both limitations.

In this section we will present sampling-based methods for quantifying the probability of satisfying arbitrarily complex floating-point constraints. We will briefly discuss how to generalize to different domains at the end of the section.
\mycomment{SHOULD TRY TO COMPRESS THE FOLLOWING A BIT}

Sampling-based methods estimate the probability of satisfying a given constraint using a Monte Carlo approach~\cite{robert2013monte}. For simplicity, we will focus on the simplest, though general, method suitable for our purpose: hit-or-miss Monte Carlo. We will add some pointer to more advanced methods later in the section.

%Sampling-based probability computations rely on the application of Monte Carlo methods for estimating the probability of satisfying a given constraint. Monte Carlo estimation is a well-developed field in Statistics, with countless applications in science and engineering~\cite{robert2013monte}. In this section, we will review the basics of some Monte Carlo estimation techniques used in probabilistic program analysis. % while in Section~\ref{sec:computingprobabilitiesOptimizations} we will show how this general techniques can be tailored to probabilistic program analysis for more efficient quantification procedures.

\paragraph{Hit-or-miss Monte Carlo.}
On a first hand, we will assume a uniform input distribution over bounded, real domains. Consider for example the constraint $x \leq -y \land y \leq x$, where both $x, y \in [-1, 1] \cap \mathbb{R}$. Taken an input $x, y$, the constraint would either be satisfied or violated. In probabilistic terms, this can be seen as a Bernoulli experiment, i.e., an experiment having only two mutually exclusive outcomes, \textit{true} or \textit{false}, where the probability of the \textit{true} outcome is the parameter $p$ of a Bernoulli distribution~\cite{pestman1998mathematical} (the probability of the \textit{false} outcome is in turn $1-p$). Our goal is to estimate the parameter $p$, from $n$ random samples over the input domain.

Figure~\ref{fig:sampling-based} plots the solution space for the example constraint ($x$ and $y$ on the x- and y-axis, respectively); the value $p$ we aim to estimate is the ratio between the shadowed area, enclosing all the points satisfying the constraint, an the input domain (i.e., the outer box).

\begin{figure}[ht]
\centering
\begin{minipage}[b]{0.45\linewidth}
\includegraphics[width=1\linewidth]{plot_sampling01}
\end{minipage}
\quad
\begin{minipage}[b]{0.45\linewidth}
\includegraphics[width=1\linewidth]{plot_sampling02}
\end{minipage}
\caption{Sampling-based solution space quantification for $x \leq -y \land y \leq x$, $x, y \in [-1, 1]$.}
\label{fig:sampling-based}
\end{figure}

The hit-or-miss Monte Carlo method consists in taking $n$ independent random samples uniformly within the domain; if a sample $s_i$ satisfies the constraint, we assign $s_i=1$, otherwise, $s_i=0$. This process is formally called a Binomial experiment with $n$ samples. The maximum likelihood estimate for $p$ is then $\hat{p}$~\cite{pestman1998mathematical}:

\begin{equation}\label{eq:mleEstimator}
	\hat{p} = \frac{\sum_{i=1}^n s_i}{n} \qquad\qquad\qquad \sigma(\hat{p}) = \sqrt{\frac{\hat{p} \cdot (1-\hat{p})}{n}}
\end{equation}
 
 
The right part of Equation~\eqref{eq:mleEstimator} shows the standard deviation $\sigma$ of $\hat{p}$~\cite{pestman1998mathematical}. The standard deviation is an index of the convergence of the estimate. Notably, it decreases with the square root of the number of samples; when the number of samples grows to infinity, the standard deviation goes to $0$, making the estimation converge to the actual value of $p$.

Despite the convergence of $\hat{p}$ to $p$ can be proved only in the limit, given the value of $\hat{p}$, its standard deviation $\sigma$, and a desired confidence level $0<\alpha<1$, it is possible to define a confidence interval for the unknown value $p$. In particular:

\begin{equation}\label{eq:mleInterval}
	Pr\Big( \hat{p} - z_{\frac{\alpha}{2}} \cdot \sqrt{\frac{\hat{p} \cdot (1-\hat{p})}{n}} \ \leq p \leq \ \hat{p} + z_{\frac{\alpha}{2}} \cdot \sqrt{\frac{\hat{p} \cdot (1-\hat{p})}{n}} \Big) = 1-\frac{\alpha}{2}
\end{equation}

\noindent where $z_{\frac{\alpha}{2}}$ is the $1-\frac{\alpha}{2}$ quantile of the standard Gaussian distribution~\cite{pestman1998mathematical}.

Equation~\eqref{eq:mleInterval} is constructed using the central limit theorem, under the assumption that a large number of samples $n$ have been collected (as a rule of thumb, hundreds of samples or more are almost surely a good fit for this assumption). The width of the interval, which is an index of the accuracy of the estimate, can be arbitrarily reduced by increasing the number of samples $n$; thus, Equation~\eqref{eq:mleInterval} can be used as stopping criteria for the estimation process.

In our example, in a run with $n=10000$ samples, we obtained $\hat{p}= 0.2512$ with standard deviation $\sigma(\hat{p})=0.00433703$; thus, with $99\%$ confidence, we can conclude $p \in [0.248126, 0.254274]$. From Figure~\ref{fig:sampling-based}, it is immediate to calculate that $p=0.25$, which falls within the computed interval.

It can be noted that hit-or-miss methods may require a large number of samples to converge to a high accuracy (small interval). This is even worse when the actual value of $p$ close to its extremes (0 or 1). Significant improvements on the convergence rates can be achieved with more complex sampling procedures, including the use of quasi-Monte Carlo sampling~\cite{robert2013monte}, or, when $p$ is close to its extremes, importance sampling, Markov Chain Monte Carlo, or slice sampling~\cite{bishop2006pattern}; some of these methods have successfully been used for similar problems in probabilistic model checking~\cite{importanceSamplingSMC,splittingSMC,statisticalModelChecking}. More accurate confidence intervals can also be used as stopping criteria. The interval in Equation~\cite{eq:mleInterval} is indeed conservative and does not exploit all the information in the estimator because of the approximation via the central limit theorem. More precise intervals have been proposed in statistics~\cite{pestman1998mathematical}; in probabilistic model checking, the most commonly used is Chernoff-Hoeffding's bound~\cite{hoeffding1963probability,approximatePMC,statisticalModelChecking}. Bayesian estimators can also be used, allowing for the inclusion of prior knowledge on the expected result (when available)~\cite{Robert2007BayesianChoice,gelman2003bayesian}; Bayesian methods demonstrated a faster convergence rate in many probabilistic verification problems~\cite{Zuliani:2010:BSM:1755952.1755987}. Finally, an hybrid approach exploiting interval constraint propagation for a compositional solution of the estimation problem has been proposed for probabilistic program analysis in~\cite{Borges2014,2015-fse-qcoral}.

\paragraph{Distribution-aware sampling.} So far we have assumed that inputs are distributed uniformly within their domain. When different distributions are specified for the input variables, the hit-or-miss Monte Carlo algorithm presented above can still be applied, though the samples from input variables should be taken according to their specified input distributions.

Efficient sampling algorithms exist for the most common continuous and discrete distributions, with off-the-shelf implementations for several programming languages (e.g.,~\cite{commonsMath3} for Java). While a survey of ad-hoc methods for random number generation is beyond the scope of this paper (the interested reader can refer, e.g., to~\cite{gentle2013random}), we will briefly recall a general sampling method that can be used to reduce sampling from arbitrary distributions to sampling from a uniform one. 

Assume our goal is to take a sample from a distribution $D$, e.g., a Gaussian distribution describing the inputs received by a temperature sensor. This distribution has a cumulative distribution function $CDF_D(x)$ representing the probability of observing a value less than or equal to $x$~\cite{pestman1998mathematical}. The value of the CDF is trivially bounded between 0 and 1, for $x\to -\infty$ and $x \to \infty$, respectively. Furthermore, assuming every possible outcome has a strictly positive probability, as is the case for most distributions used in practice, the CDF is strictly monotonic and invertible; let us denote its inverse $CDF_D^{-1}(\cdot)$. A general method to reduce sampling from $D$ to sampling from a Uniform distribution is called \emph{inverse CDF sampling} or \emph{inverse transform sampling} and is composed of the following three steps:

\begin{enumerate}
	\item generate a random sample $u$ from the Uniform distribution on $[0,1]$
	\item find the value $x$ such that $CDF_D(x)=u$, i.e., $CDF_D^{-1}(u)$
	\item return $x$ as the sample from $D$
\end{enumerate}

For most practically used distributions, the inverse CDF can be computed efficiently, allowing more general input distributions to be used in sampling-based probability computation methods. Slight variations of inverse CDF sampling allow you to additionally sample from truncated distributions~\cite{cohen1991truncated}, i.e., when a probability distribution is restricted to a specific domain (e.g., the input temperature has a Gaussian distribution with a certain mean and variance, but is restricted to the range $[0,100]$), which is common in probabilistic program analysis. 

In some cases, the input variables are not probabilistically independent. For example, the temperature and the altitude measured by input sensors of an airplane might be correlated with one another, since higher altitudes usually correspond to lower temperatures. Whenever variables are not independent, their values have to be sampled as tuples from a multidimensional probability distribution. While the sampling approach discussed so far is also conceptually applicable to multidimensional distributions, it is usually computationally more expensive to compute the CDF and its inverse in higher dimensions. More sophisticated sampling techniques are preferred in this case, such as Gibbs sampling~\cite{Robert2005MCBook}.

As a final remark, while discretization can be used to obtain arbitrarily accurate approximations of complex distributions (over both integer and real numbers), the complexity of the probability computation method described in Section~\ref{sec:computingprobabilitiesExact} is exponential in the resolution of discretization, e.g., if a program has 5 input variables and the domain of each of them is split into 3 intervals, the size of the domain partition is $5^3$. When the number of variables grows, obtaining fine grained discretization of their probability distributions may become computationally too expensive. Distribution-aware sampling can be leveraged to avoid both this exponential explosion and the need to define a priori a discretization resolution~\cite{2015-fse-qcoral}.


\paragraph{Beyond floating-point numbers.}
The framework of sampling-based probability computation described in this section is theoretically applicable to domains other than purely numeric ones. However, its practical implementation requires the ability to generate random inputs from these domains, possibly according to the input distributions specified by the developers. Random test case generation has developed effective techniques to generate random inputs for realistic programs. However, most of the techniques developed in this area do not provide any guarantee on the distribution of the generated inputs, possibly biasing the estimation process. 

Sampling-based methods have been proposed for model counting of SAT problems (e.g., in~\cite{satCounting01,biere2009handbook,journalscorrMeel14}, also with distribution-aware approaches~\cite{chakraborty2014distribution}) and SMT problems (e.g., in~\cite{countingSMT}), while stochastic grammars can be used to generate random strings according to specified distributions~\cite{stochasticGrammars}.



%	\begin{itemize}
%		\item Sampling from uniform distributions (base case, best we can if no input distribution is available; based on classic probability, i.e., count success over total)
%		\item Discretization of non-uniform distributions (useful when a finite number of usage scenarios are available; can approximate any distribution with arbitrary accuracy; most straightforward when inferring distributions from past executions, i.e., histograms; it does not scale for fine-grained approximations)
%		\item Distribution-aware sampling (quite straightforward for distributions over numerical domains; requires more complex, unbiased, input generation techniques when sampling from other domains, e.g., data structures, but similar to random testing; scalability issues when sampling correlated input variables)
%      
%		\item Monte Carlo methods
%			\begin{itemize}
%				\item Hit or miss Monte Carlo
%				%\item \mycomment{Anto: Mention Crude montecarlo for integration? Gibbs and MCMC sampling will be just mentioned; especially MCMC is used by the MSR guys}
%				\item Frequentist and Bayesian estimators (used in prob. model check.)
%					\begin{itemize}
%						\item historically frequentist first, with static bounds (based on Chernoff's) for prob MC; then sequential ratio tests, still frequentist.
%						\item Bayesian from CMU
%					\end{itemize}
%				\item The variance issue and convergence acceleration techniques (just a paragraph with some refs):
%					\begin{itemize}
%						\item Importance sampling (used in prob. model check.)
%						\item Importance slicing (used in prob. model check.)
%					\end{itemize}
%
%				\item Exploiting he law of total probability for composing different estimators: Interval constraints propagation (PLDI14) and statistical/exact (FSE14)
%				\item Dealing with nondeterminism (freq and bayesian in prob MC, ASE14)	
%			\end{itemize}
%			
%			\item approximate $\sharp$-sat and $\sharp$-smt (\url{http://arxiv.org/pdf/1306.5726v3.pdf}, )
%	\end{itemize}


\ignore{
\input{computing_prob_optimizations}
}






