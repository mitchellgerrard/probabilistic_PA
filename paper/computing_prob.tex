\section{Computing Program Probabilities}
\label{sec:computingprobabilities}

\mycomment{Matt: it would be good if there was some discussion about
how to handle a given distribution.  For example, how would prob
sym exe handle a call to a function to return a value from N(0,1)
(a normal distribution with mean 1 and standard distrubution 0).
Equivalently how would this be specified as a usage profile.
Is there something better than relying on a person to write this down?
I know there is tons of work published on this, but is there a simple
approach we can describe or that you've used.}


Computing probabilities for probabilistic program analysis can usually be reduced to computing the probability of satisfying a boolean constraint over the program variables. In this section we will introduce some of the basic techniques currently used in program analysis. 

To simplify the notation, we will focus on implicit probabilistic constructs, assuming the program under analysis has input variables $V=\{v_1, v_2, \dots, v_n\}$, where $v_i$ has domain $d_i$ and comes with a probability distribution $\mathcal{P}_i: d_i \to [0, 1]$. The input domain $D$ is defined as the cartesian product of the domains $d_i$, while the input distribution $\mathcal{P}$ is defined as the joint distribution over all the input variables $\prod_i \mathcal{P}_i(\bar{v_i})$. Given a constraint $\phi: V \to \{true, false\}$, our goal is to compute the probability $Pr(\phi)$ of satisfying $\phi$ given the input domains and probability distributions. This problem is usually referred to as model counting, when the input domains are countable, or solution space quantification, when the input domains are modeled as uncountable (e.g., abstracting floating-point numbers as reals).

\mycomment{Anto: Section outline to be added}

\mycomment{Anto: check definitions of model counting and solution space quantification}


%Some techniques allow for bounding the probability value within a certain interval; 

\subsection{Exact and numeric computation}\label{sec:computingprobabilitiesExact}

% OUTLINE
%	\begin{enumerate}
%		\item Finite domains
%			\begin{enumerate}
%				\item Linear integers (Latte, Barvinok, Omega for negation; used in our works)
%				\item Strings (bounds: \url{http://www.comp.nus.edu.sg/~shweta24/publications/smc\_pldi14.pdf} ; automata-based exact and upperbounds: \url{http://www.cs.ucsb.edu/~bultan/publications/model-counting.pdf}; we should also check the related work thereof)
%				\item Data structures (icse13 and spin15)
%				\item Sat and smt (\url{http://arxiv.org/pdf/1306.5726v3.pdf}, \url{http://arxiv.org/pdf/1411.0659.pdf}; these papers should have been published, we should check related work in there)
%			\end{enumerate}
%		
%		\item Uncountable domains
%			\begin{enumerate}
%				\item Symbolic and numerical integration (interesting, but symbolic does not scale apart from a few simple cases, while numeric suffers for large cardinality of input domains; however: general, available off-the-shelf, parallelizable for numerical)
%			\end{enumerate}
%	\end{enumerate}

\paragraph{Finite domains.} 

If the input domain is finite, classic probability can be used to reduce the computation of $Pr(\phi)$ to a counting problem (let us on a first hand assume a uniform distribution over all the possible input values, i.e., each valid input has the same probability):
%
\begin{equation}\label{eq:counting}
	Pr(\phi) = \frac{\sharp(\phi \land D)}{\sharp(D)}
\end{equation}
%
\noindent where $\sharp(\cdot)$ counts the number of inputs satisfying the argument constraint, $D$ has been overloaded to represent the finite domain as a constraint ($\sharp(D)$ is a short form for the size of the domain)\footnote{More precisely, Equation~\eqref{eq:counting} represents the probability of satisfying the constraint $\phi$ conditioned to the fact that the input is within the prescribed domain $D$.}. For example, considering a single integer input variable $x$ taking values between $1$ and $10$ uniformly, $\sharp(D)=10$ and $\sharp(x\leq5 \land D)=5$ leading to a probability of $.5$ of satisfying the constraint.

An efficient implementations of $\sharp(\cdot)$ are available for several classes of model counting problems:

\begin{itemize}
	\item \textbf{Linear integer arithmetics}: the conjunction of linear constraints over a finite integer domain geometrically defines a multi-dimensional lattice bounded by a convex polytope~\cite{de2008computationalGeometry}. To count the number of points composing this structure, an efficient solution has been proposed by Barvinok in~\cite{barvinok1994polynomial}. This algorithm is grounded on the construction of generating functions suitable for solving the counting problem in polynomial time with respect to the number of variables and the number of constraints. Notably, besides the number of bits required to represent the numerical values, the complexity of this algorithm does not depend on the actual size of the variable domains. This makes the computation feasible for very large input domains, allowing its application to probabilistic program analysis. Several implementations of this algorithm are available, the two most popular being LattE~\cite{LattESoftware} and Barvinok~\cite{verdoolaegesoftware}. When disjunctions appear in the constraint, these have to be preprocessed before applying Barvinok's algorithm (e.g., using the Omega library~\cite{Omega1996}). This normalization increases the overall complexity of model counting, however, several optimizations can be leveraged to reduce the computational effort (we will discuss some later in Section~\ref{sec:computingprobabilitiesOptimizations}). Barvinok algorithm has been used for probabilistic program analysis in \cite{Geldenhuys2012,Filieri2013,Filieri2015}.

	\item \textbf{Bounded data structures}: data structures are usually composed by a structural dimension, e.g., lists or tree, and by a payload stored in each node of the structure. The level of decoupling between structure and payload differs case by case, for example a list of integers may be sorted or not. Early work on complexity analysis explored the use of generating functions for representing the number of valid instances of a given data structure without explicitly enumerating them (e.g.,~\cite{flajolet1985mathematical}). However, despite its computational efficiency, this approach requires a significant amount of human work because the construction of these generating functions can hardly be automatized from source code. For the sake of generality, early work in probabilistic program analysis~\cite{Filieri2013} used enumeration-based approaches, such as Korat~\cite{Korat2002}. This technique relies on the formalization of both the invariants characterizing the valid structures and of the constraints to be counted as executable boolean methods, and then generates all the instances for which these methods return true. The generation is enhanced with smart pruning techniques to reduce the actual exploration space, although their complexity still leaves most realistic programs out of reach. A recent approach proposed in~\cite{Filieri2015} decouples the structural part and the payload, employing the partial enumeration of the former and the use of more efficient model counting techniques for the latter, whenever possible. For example, an acyclic sorted list of integers between 1 and 10 having at most 3 elements would require the enumeration of 4 different structural configuration (including the empty list) and the evaluation of 8 linear integer constraints (which can be computed with Barvinok's algorithm), instead of exploring all 1111 possible instances.

	\item \textbf{Regular expressions}: a variety of practically relevant constraints on strings can be formalized as matching with a regular expression~\cite{Luu2014,Aydin2015}. If the (maximum) length of the string is bounded, the number of instances matching a regular expression can be counted exactly with an automata-based approach. The regular expression is first transformed into the corresponding accepting automata. Then, an exponential generating function is automatically constructed to count the accepting runs of the automata up to a certain length, without the need to enumerate all of them explicitly. The technique is actually more general and can be used for any constraint whose satisfaction can be reduced to counting the accepting runs of a finite state automata. For more general constraints, called pseudo-relational, the exact count is not computable, though its value can be bounded in a finite interval allowing, in some cases, for best- or worst-case analysis~\cite{Aydin2015}.
	
	\item \textbf{SAT and SMT}: \mycomment{TBD (\url{http://arxiv.org/pdf/1306.5726v3.pdf}, \url{http://arxiv.org/pdf/1411.0659.pdf}; these papers should have been published, we should check related work in there)}

\end{itemize}


\paragraph{Floating-point numbers.} 
Floating-point numbers are usually abstracted as real numbers for analysis purposes. Computing the probability of satisfying a constraints over reals requires refining Equation~\ref{eq:counting} to cope with the density of the domain. In particular, the counting function $\sharp(\phi)$ has to be replaced by the integration of an indicator function on $\phi$, i.e., a function returning $1$ for all the inputs satisfying $\phi$ \cite{Borges2014}. This integration can be performed exactly only for a limited number of cases, where symbolic integration is possible. For all the other cases, only numerical integration is possible. A number of commercial and open-source tools can be used for this purpose, however 1) numerical computations are accurate up to a certain bound, and 2) they do not scale when the cardinality of the input domain grows. In these cases sampling-based methods are preferable. 

\paragraph{Handling input distributions.} 
For finite domains, we assume, without loss of generality, the input distribution to be specified on a finite partition $D^1, D^2, \dots, D^n$ of the input domain $D$ (i.e., $\cup_i D^i \equiv D$ and $D^i \cap D^j \neq \emptyset \implies i=j$) via the probability function $Pr(D^i)$. We assume elements within the same set $D^i$ to have the same probability. The case of uniform distribution described so far corresponds to the partition with cardinality 1, i.e., the whole domain.
 
Since the elements of the partition are disjoint by construction, we can exploit the law of total probability to extend Equation~\eqref{eq:counting} to include the information about the input distribution:
%
\begin{equation}\label{eq:countingInputDistribution}
	Pr(\phi) = \sum_i \frac{\sharp(\phi \land D^i)}{\sharp(D^i)} \cdot Pr(D^i)
\end{equation}
%
\noindent where $D^i$ has again been overloaded to represent the constraint of an element belonging to $D^i$.

Formalizing the input distribution on a finite partition of the input domain is general enough to capture every valid distribution on the inputs, including possible correlations or functional dependencies among the input variables. However, the finer the specification of the input distribution, the more complex the computation of Equation~\eqref{eq:countingInputDistribution}, which, in the worst case, may require the computation of $|D|$ addends \cite{Borges2014}. While this worst case is unlikely to occur (also due to the optimization strategies that will be described later), more efficient probability computations are possible employing distribution-aware sampling-based methods, which will be described in the next section.

\subsection{Sampling-based methods}\label{sec:computingprobabilitiesSampling}
Exact methods may suffer two main limitations: 1) generality with respect to input domains and constraint classes and 2) scalability, either due to intrinsic complexity of their algorithm or to induced by the discretization of the input distributions. In many cases, sampling-based methods may be used to leverage both limitations.

In this section we will present sampling based methods for quantifying the probability of satisfying arbitrarily complex floating-point constraints. We will briefly discuss how to generalize to different domains at the end of the section.

Sampling-based probability computation rely on the application of Monte Carlo methods for estimating the probability of satisfying a given constraint. Monte Carlo estimation is a well developed field in Statistics, with countless applications in science and engineering XXX. In this section we will review the basics of Monte Carlo estimation while in Section~\ref{sec:computingprobabilitiesOptimizations} we will show how this general techniques can be tailored to probabilistic program analysis for more efficient quantification procedures.

\paragraph{Hit-or-miss Monte Carlo.}

As in the previous section, we will first introduce this quantification technique assuming a uniform input distributions over a convex domain. Consider for example the constraint $v_1 \leq -v_2 \land v_2 \leq v_1$, where both $v_1, v_2 \in [-1, 1] \cap \mathbb{R}$. For each value of $v_1$ and $v_2$, the constraint can be either true or false. To estimate the probability of satisfying the constraint (and in turn of violating it), we can define a suitable probabilistic model and then run a set of experiments to estimate its parameters.

A suitable probabilistic model to represent the binary true/false outcome of an experiment is a Bernoulli distribution XXXX. This is a parametric distribution with a single parameter, let us call it $p$, representing the probability of observing the outcome true; the complementary outcome, false, will have probability $1-p$.

The simplest way to estimate $p$ consists in generating $n$ independent random values for $v_1$ and $v_2$ and count how many of those values satisfy the constraint. More precisely, we can define an estimator $\hat{P}$ for the parameter $p$ which is the ratio between the number of experiments returning true and the total number of experiments, $n$. The most important properties of this estimator (that can be justified, for example, by the central limit theorem XXXX) are its expected value $E[\hat{P}]$, which is the most likely estimate, and its variance $Var[\hat{P}]$, which is an index of the uncertainty we have about our estimate after the experiments. Formally:
%
\begin{equation}\label{eq:mleEstimator}
	E[\hat{P}] = \bar{x} \qquad Var[\hat{P}] = \frac{\bar{x} \cdot (1-\bar{x})}{n}
\end{equation}
%
\noindent where $\bar{x}=\sum_{i=1}^n x_i$ and $x_i$ is $1$ if the $i$-th experiment retuned true and $0$ otherwise.

The estimator $\hat{P}$ has tow important properties: it is unbiased, i.e., its expected value converges to the actual probability of satisfying the constraints when the number of experiments grows; it is consistent, i.e., its variance, and in turn our uncertainty about the result, decreases to 0 (as can be noted by the denominator $n$ in Equation~\eqref{eq:mleEstimator}).

From the perspective of probabilistic program analysis, these two properties can be used to guarantee the asymptotic correctness of the computed probability, as well as to quantify the convergence of the estimation process. The variance can indeed be used to provide probabilistic guarantees on the correctness of the result in the form of confidence intervals XXX (assuming $n$ is large enough):

\begin{equation}
	Pr\Big( \bar{x} - z_{\frac{\alpha}{2}} \cdot \sqrt{\frac{\bar{x} \cdot (1-\bar{x})}{n}} \ \leq p \leq \ \bar{x} + z_{\frac{\alpha}{2}} \cdot \sqrt{\frac{\bar{x} \cdot (1-\bar{x})}{n}} \Big) = 1-\frac{\alpha}{2}
\end{equation}

\noindent where $z_{\frac{\alpha}{2}}$ is the $1-\frac{\alpha}{2}$ quantile of the standard Normal distribution XXX. For example, if we want to have a confidence of $99.9\%$ on our estimation, we will pick $z_{\frac{\alpha}{2}}=3.08$ and keep collecting samples until the length of the interval is acceptably small, i.e., the accuracy of the result is acceptable.

XXXXParagraph on other bounds, hypothesis testing and Bayesian methods.


\paragraph{Distribution-aware sampling.}

\paragraph{Beyond floating-point numbers.}


%	\begin{itemize}
%		\item Sampling from uniform distributions (base case, best we can if no input distribution is available; based on classic probability, i.e., count success over total)
%		\item Discretization of non-uniform distributions (useful when a finite number of usage scenarios are available; can approximate any distribution with arbitrary accuracy; most straightforward when inferring distributions from past executions, i.e., histograms; it does not scale for fine-grained approximations)
%		\item Distribution-aware sampling (quite straightforward for distributions over numerical domains; requires more complex, unbiased, input generation techniques when sampling from other domains, e.g., data structures, but similar to random testing; scalability issues when sampling correlated input variables)
%      
%		\item Monte Carlo methods
%			\begin{itemize}
%				\item Hit or miss Monte Carlo
%				%\item \mycomment{Anto: Mention Crude montecarlo for integration? Gibbs and MCMC sampling will be just mentioned; especially MCMC is used by the MSR guys}
%				\item Frequentist and Bayesian estimators (used in prob. model check.)
%					\begin{itemize}
%						\item historically frequentist first, with static bounds (based on Chernoff's) for prob MC; then sequential ratio tests, still frequentist.
%						\item Bayesian from CMU
%					\end{itemize}
%				\item The variance issue and convergence acceleration techniques (just a paragraph with some refs):
%					\begin{itemize}
%						\item Importance sampling (used in prob. model check.)
%						\item Importance slicing (used in prob. model check.)
%					\end{itemize}
%
%				\item Exploiting he law of total probability for composing different estimators: Interval constraints propagation (PLDI14) and statistical/exact (FSE14)
%				\item Dealing with nondeterminism (freq and bayesian in prob MC, ASE14)	
%			\end{itemize}
%			
%			\item approximate $\sharp$-sat and $\sharp$-smt (\url{http://arxiv.org/pdf/1306.5726v3.pdf}, )
%	\end{itemize}





\subsection{Optimizations}\label{sec:computingprobabilitiesOptimizations}

\paragraph{Divide and conquer.}

\paragraph{Sampling effort allocation.}

\paragraph{Interval constraint propagation.}

Divide and conquer from icse13 and and sensitivity-aware sampling from fse15
