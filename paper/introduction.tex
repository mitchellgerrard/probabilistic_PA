\section{Introduction}
\label{sec:introduction}

Static program analyses aim to calculate properties of 
the possible executions of a program without ever running the program
and have been an active topic of study since the 1960s \cite{First}.
Initially developed to allow compilers to generate more efficient
output programs by the mid-1970s \cite{Fosdick} researchers had
understood that such program analyses could be applied to fault
detection -- and verification of the absence of specific classes of faults.

There are extremely well-developed frameworks for defining
and implementing such analyses.  In this paper we focus on three
such frameworks: data flow analysis, model checking, and symbolic execution.

The power of these analysis techniques, and what distinguishes them from
simply running a program and observing its behavior, is in their
ability to reason about program behavior without knowing all of the
exact details of program execution, e.g., the specific 
input values provided to the program, the set of operating system
thread scheduler decisions.  This tolerance of uncertainty allows analyses
to provide useful information when users don't know exactly how
a program will be used (e.g., when a program is first released, when
embedded systems read sensor inputs from the physical world, or
when it is ported to an operating system with a different scheduler).

Static analyses model uncertainty in program behavior
through the use of various forms of abstraction and symbolic representation.
For example, symbolic expressions with associated logical constraints 
are used, in symbolic execution, to define abstract domains
in data flow analysis, and for predicate abstraction in model checking, 
to capture sets of data values that may be input to a program.
Non-deterministic choice is another widely used approach for modeling
uncertainty -- for instance in modeling uncertain branch 
decisions in data flow analysis and
in scheduler decisions in model checking.
While undeniably effective, these approaches sacrifice potentially
important distinctions in program behavior.   

Consider a program that accepts an integer input representing
a person's income.  A static analysis might reason about the program
allowing any integer value or, perhaps, by applying
some simple assumption, i.e., that income must be non-negative.
Domain experts have studied income distributions and find that
it varies according to a generalized beta distribution 
\cite{IncomeDistribution}.  Can this type of information be 
exploited in a program analysis to speed analysis,
to yield more useful analysis results, or to reason 
about new types of program properties?

For decades there has been a growing awareness of the value of 
incorporating more precise forms of uncertainty into program behavior.  
The field of randomized algorithms has studied how to incorporate
randomness, as an additional program input, as a means of achieving
good average case performance -- and consequently as a defense against
intolerable worst case performance.
Programming such algorithms requires that primitives be available
to draw values from probability distributions and there are many
languages that provide such primitives \cite{NetLogo,others}.  

Regardless of whether information about the distribution of
values is embedded within a program or stated as an input assumption,
the semantics of these probabilistic programs is well-understood --
and has long been studied \cite{Kozen,others}
\footnote{In recent
years, the term probabilistic program has been generalized beyond
drawing inputs from probability distributions, which we
consider here, to programs that can condition program behavior -- by
rejecting certain program runs -- and thereby be viewed as
computations over probability distributions.  We refer the reader to the
recent paper by Gordon, Henzinger, Nori and Rajamani \cite{ICSE2014}
for discussion of these more general programs.}. 
What has lagged behind is work developing frameworks for 
defining and implementing static analysis techniques for such programs.

What would such analyses have to offer?
\mycomment{Matt: add the laundry list of questions that might be
posed.   reliability, probability of failure, program equivalence,
a new coverage measure, more informative than a failed sound analysis
(i.e., saying a property fails to hold just means Pr<1), ...}

In this paper, we survey work on adapting model checking, data flow analysis, 
and symbolic execution, to consider probabilistic information.
We begin with a brief background that provides basic definitions
related to static analysis and probabilities.
Section~\ref{sec:computingprobabilities} discusses approaches that
have been developed to reason about the probability of program
related events, e.g., executing a path, taking a branch, or reaching a state.
The following three sections, 
Section~\ref{sec:pmc}-\ref{sec:pse}, survey work on probabilistic
model checking, probabilistic data flow analysis, and probabilistic
symbolic execution.  These sections seek to expose similarities
and differences among analysis approaches with respect to issues
such as the \texttt{accuracy} of analysis results, i.e., how results
are related to executable program behavior, 
the aspects of behavior that are governed by probabilities, and
the treatment of non-determinism -- which is a standard modeling
approach in static-analysis.
Section~\ref{sec:probspecs} surveys approaches to the cross-cutting
issue of how probabilities can be specified or encoded for use
in analyses.
Finally, we conclude with the discussion of a series of open questions
and research challenges that we believe are worth pursuing.

