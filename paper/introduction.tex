\section{Introduction}
\label{sec:introduction}

Static program analyses aim to calculate properties of 
the possible executions of a program without ever running the program
and have been an active topic of study for over 4 decades.
Initially developed to allow compilers to generate more efficient
output programs, by the mid-1970s \cite{fosdick1976data} researchers had
understood that such program analyses could be applied to fault
detection -- and verification of the absence of specific classes of faults.

There are extremely well-developed frameworks for defining
and implementing such analyses.  In this paper we focus on two
such frameworks: data flow analysis and symbolic execution.

The power of these analysis techniques, and what distinguishes them from
simply running a program and observing its behavior, is in their
ability to reason about program behavior without knowing all of the
exact details of program execution, e.g., the specific 
input values provided to the program, the set of operating system
thread scheduler decisions.  This tolerance of uncertainty allows analyses
to provide useful information when users don't know exactly how
a program will be used (e.g., when a program is first released, when
embedded systems read sensor inputs from the physical world, or
when it is ported to an operating system with a different scheduler).

Static analyses model uncertainty in program behavior
through the use of various forms of abstraction and symbolic representation.
For example, symbolic expressions are used to encode logical constraints 
in symbolic execution, to define abstract domains
in data flow analysis, and to capture sets of data values that may be input
to a program in the predicate abstraction of model checking, 
to capture sets of data values that may be input to a program.
Non-deterministic choice is another widely used approach for modeling
uncertainty -- for instance in modeling uncertain branch 
decisions in data flow analysis and
in scheduler decisions in model checking.
While undeniably effective, these approaches sacrifice potentially
important distinctions in program behavior.   

Consider a program that accepts an integer input representing
a person's income.  A static analysis might reason about the program
allowing any integer value or, perhaps, by applying
some simple assumption, i.e., that income must be non-negative.
Domain experts have studied income distributions and find that
it varies according to a generalized beta distribution 
\cite{mcdonald1984some,thurow1970analyzing}.  Can this type of information be 
exploited to yield useful analysis results when classic
analyses fail, or to reason about new types of program properties?

For decades there has been a growing awareness of the value of 
incorporating more precise forms of uncertainty into program behavior.  
The field of randomized algorithms has studied how to incorporate
randomness, as an additional program input, as a means of achieving
good average case performance -- and consequently as a defense against
intolerable worst case performance.
Programming such algorithms requires that primitives be available
to draw values from probability distributions and there are many
languages that provide such primitives, e.g., NetLogo \cite{tisue2004netlogo},
the C++ \texttt{$\mathtt{<random>}$} library, and the GNU Scientific library.

Regardless of whether information about the distribution of
values is embedded within a program or stated as an input assumption,
the semantics of these probabilistic programs is well-understood --
and has long been studied 
\cite{kozen1981semantics,kozen1983probabilistic,jones1990probabilistic,morgan1996probabilistic}
\footnote{In recent
years, the term probabilistic program has been generalized beyond
drawing inputs from probability distributions, which we
consider here, to programs that can condition program behavior -- by
rejecting certain program runs -- and thereby be viewed as
computations over probability distributions.  We refer the reader to the
recent paper by Gordon, Henzinger, Nori and Rajamani \cite{Gordon2014}
for discussion of these more general programs.}. 
What has lagged behind is work developing frameworks for 
defining and implementing static analysis techniques for such programs.

What would such analyses have to offer?
They can, of course, provide a means of analyzing programs that compute
with values chosen from probability distributions, but they offer much
more.
For example, researchers have explored the use of probabilistic analysis
results to assess the security of software components~\cite{probabilisticPolyhedra},
assess program reliability~\cite{FilieriICSE13}, as a measure of program
similarity (or inequivalence)~\cite{GeldenhuysISSTA12}, as a characterization 
of the coverage
achieved by an analysis technique~\cite{DwyerASE11}, and to provide information
about program properties when a classic analysis would fail, e.g.,
by running out of memory, time, or due to excessive approximation.

In this paper, we survey work on adapting data flow analysis 
and symbolic execution to use probabilistic information.
We begin with a brief background that provides basic definitions
related to static analysis and probabilities.
Section~\ref{sec:overview} attempts to expose some of the key
intuitions and concepts that cross-cut the work in this area.
Section~\ref{sec:computingprobabilities} discusses approaches that
have been developed to reason about the probability of program-related 
events, e.g., executing a path, taking a branch, or reaching a state.
The following two sections, \ref{sec:pdfa}-\ref{sec:pse}, 
survey work on probabilistic data flow analysis and probabilistic
symbolic execution.  
Section~\ref{sec:probspecs} surveys approaches to the cross-cutting
issue of how probabilities can be specified or encoded for use
in analyses.
We conclude with the discussion of a series of open questions
and research challenges that we believe are worth pursuing.

