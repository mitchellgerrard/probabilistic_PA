\section{Introduction}
\label{sec:introduction}

Static program analyses calculate properties of 
the possible executions of a program without ever running the program,
and have been an active topic of study for over five decades.
Initially developed to allow compilers to generate more efficient
output programs, by the mid-1970s \cite{fosdick1976data} researchers 
understood that program analyses could be applied to fault
detection and verification of the absence of specific classes of faults.

The power of these analysis techniques, and what distinguishes them from
simply running a program and observing its behavior, is their
ability to reason about program behavior without knowing all of the
details of program execution (e.g., the specific 
input values provided to the program).
This tolerance of uncertainty allows analyses
to provide useful information when users don't know exactly how
a program will be used.

Static analyses model uncertainty 
through the use of various forms of abstraction and symbolic representation.
For example, symbolic expressions are used to encode logical constraints 
in symbolic execution~\cite{king1976symbolic}, to define abstract domains
in data flow analysis~\cite{kildall1973unified,cousot1977abstract}, and to 
capture sets of data values that constitute reachable states via
predicate abstraction~\cite{graf1997predabs}.
Nondeterministic choice is another widely used approach,
for instance, in modeling branch decisions in data flow analysis.
While undeniably effective, these approaches sacrifice potentially
important distinctions in program behavior.   

Consider a program that accepts an integer input representing
a person's income.  A static analysis might reason about the program
by allowing any integer value, or, perhaps, by applying
some simple assumption, i.e., that income must be non-negative.
Domain experts have studied income distributions and find that
incomes vary according to a generalized beta distribution 
\cite{mcdonald1984some,thurow1970analyzing}.  
With such a distribution the program can now be viewed as a 
\textit{probabilistic program} 
and, beginning with Kozen's seminal work in the early 1980s,
the semantics of such programs has long been studied 
\cite{kozen1981semantics,kozen1983probabilistic,jones1990probabilistic,morgan1996probabilistic}.

For non-probabilistic programs, it was just over six years
from Floyd's foundational work on program semantics~\cite{FLOYD67} 
to Kildall's widely-applicable static analysis framework~\cite{kildall1973unified}.
Sophisticated extensions of Kildall's work are prevalent 
today, e.g., ~\cite{lam11:_soot_java,Lattner:2004:LLVM}, and form 
the basis for modern software development environments.
For probabilistic programs, however, the development of 
static analysis frameworks has taken decades and 
they have not yet reached the level of applicability of 
their non-probabilistic counterparts.

What would such analyses have to offer?
Researchers have explored the use of probabilistic analysis
results to assess the security of software 
components~\cite{mardziel2013dynamic} and to measure side-channel 
leakage~\cite{CSF16,FSE16},
to assess program reliability~\cite{Filieri2013}, to measure program
similarity~\cite{Geldenhuys2012}, 
to characterize fault propagation~\cite{RiskAwareTransformation},
and to characterize the coverage
achieved by an analysis technique~\cite{DwyerASE11}.
\mycomment{Corina: add some more examples here about your latest work}
We believe that there are many more applications for cost-effective
and widely-applicable static analysis frameworks for probabilistic
programs.

In recent years, the term ``probabilistic program'' has been generalized 
beyond Kozen's definition in which programs draw inputs from probability 
distributions.
This more general setting permits the conditioning of program behavior by
allowning certain program runs to be rejected.  
These programs can be viewed as expressing computations over 
probability distributions rather than inputs drawn from a distribution.  
While recent work has just begun to explore the foundations of analysis for
this more general setting~\cite{claret2013bayesian,Gordon2014}, 
in this paper we consider
Kozen's original definition and analysis frameworks targetting such programs.

More specifically, we survey work on adapting data flow analysis 
and symbolic execution to use information about input distributions.
We begin with background that provides basic definitions
related to static analysis and probabilistic models.
Section~\ref{sec:overview} exposes some of the key
intuitions and concepts that cross-cut the work in this area.
The following two sections, \ref{sec:pdfa}-\ref{sec:pse}, 
survey work on probabilistic data flow analysis and probabilistic
symbolic execution.  
While we focus on analysis of imperative programs, we note that
principles exposed in our survey apply to analysis frameworks 
for functional programs as well.
Section~\ref{sec:computingprobabilities} discusses approaches that
have been developed to reason about the probability of program-related 
events, e.g., executing a path, taking a branch, or reaching a state.
We conclude with a set of open questions
and research challenges that we believe are worth pursuing.

