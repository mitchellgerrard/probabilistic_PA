\subsection{Scope and Background}
\label{sec:background}

This paper focuses on programs that draw input variables
from given probability distributions, or, equivalently, that make
calls on functions returning values drawn from given distributions.
The left side of Figure~\ref{fig:example} shows a method,
\texttt{m}, that we will use to illustrate concepts in this paper.     
It takes an integer variable, \texttt{x}, as input, then based on the 
results of drawing values from a Bernoulli distribution,
it either performs its computation
(which is unspecified and denoted with \texttt{...}) or triggers
an assertion. 
For the example, we might be interested in reasoning about the lack
of assertion violations.

\input{example1}

\subsubsection{Programs and Program Analyses}

There are many different ways to represent the execution behavior
of a program to facilitate analysis.  Immediately to the right of
the code in Figure~\ref{fig:example}, we show the control flow graph (CFG),
which explicitly represents control successor relationships between
statements.  A CFG models choice among successors as nondeterministic
choice -- depicted by the lack of labels on the edges.

We will also consider models that include probabilistic choice,
e.g., defining the probability that a branch is taken.
The left side of Figure~\ref{fig:choice} shows edge probabilities
that reflect the outcome of the Bernoulli draw on line 2. Thus the probability 
of taking the {\em then} branch is 0.5; the probability of taking 
the {\em else} branch is also 0.5.
In addition, we will consider models where the
choice of successor is defined by the semantics
of the branch condition.
The right side of Figure~\ref{fig:choice} 
shows a logical condition that 
reflects the fact that the value of parameter $x$ must be
less than or equal to $60$ for control to traverse the true branch at line 4.

\input{example2}

A key concept in the program analysis frameworks we survey is
\textit{symbolic abstraction}.  A symbolic abstraction is a 
representation of a set of states.  Abstractions can be encoded
in a variety of forms, e.g., logical formulae \cite{thakur2012bilateral}, binary
decision diagrams \cite{bryant1992symbolic}, or custom representations \cite{bagnara2008parma}.  For example, the set of negative integer values 
can be defined by a predicate 
$lt0 \equiv \lambda x.x<0$ which returns true for all values in the set.
Logical combinations of such predicates can be used to define
an abstract domain, ${\cal A}$, whose elements describe sets of
possible states of the program.

While abstractions encode sets of states, \textit{abstract transformers}
compute the effect of a program statement on a set of states.  For example,
the fact that the sum of any pair of negative values is negative is
encoded as $lt0 +^\# lt0 = lt0$, where $\#$ denotes an abstract transformer
for $+$ that operates on symbolic encodings of sets.

Analyses that seek to prove the satisfaction of properties generally
define abstractions that \textit{overapproximate} the set of program
states, whereas those that seek to falsify properties generally define
abstractions that \textit{underapproximate} the set of program states.

\paragraph{Data Flow Analysis}
Data flow analysis~\cite{kildall1973unified} 
provides a framework for computing properties shared by sets of
program traces reaching a program state.  It is
common for such analyses to group together the states that share a
common control location; the computed properties attempt to characterize
the invariants over those states.

Data flow analyses are solved using a fixpoint computation which
allows properties of all program paths to be safely approximated.
Model checking~\cite{clarke1999model} is a popular verification technique
which also relies on an underlying
fixpoint computation.  Moreover, data flow
analyses operate on symbolic abstractions of program states that
can be defined by abstract interpretation~\cite{cousot1977abstract}. 
In fact, it is now well-understood that data flow analysis can be
viewed as model checking of abstract interpretations~\cite{schmidt1998data}.

An abstract interpretation is a 
non-standard interpretation of program executions over an abstract domain.  
The semantics of program statements are lifted to operate
on a set of states, encoded as an element of the abstract domain,
rather than on a single concrete state.  
Generating the set of traces for non-trivial programs is impractical;
instead, abstract states can be combined, via a meet operation, wherever
traces merge in the control flow, and loops are processed
repeatedly to compute the maximum fix point (MFP).

Data flow analysis tools and toolkits exist for popular 
languages, e.g., \cite{vallee1999soot,fink2012wala,Lattner:2004:LLVM},
and have been used primarily for program optimization and
verifying program conformance with assertional specifications.

\paragraph{Symbolic Execution}
Like data flow analysis, symbolic execution~\cite{king1976symbolic,clarke1976system} 
performs a non-standard interpretation of program executions using 
a symbolic abstraction of program states.
Symbolic execution records symbolic expressions encoding the
values of program memory for each program location.  
A \textit{path condition}
accumulates symbolic expressions that encode branch constraints 
taken along an execution.

Sequences of program statements are interpreted by applying the operation
at each program location to update the values of program
variables with expressions defined over symbolic variables.
An operation that reads from an input generates a fresh symbolic
variable which represents the set of possible input values.
When a branching statement is encountered, the symbolic expression, $c$, 
encoding the branch condition is computed and a check is performed
to determine whether the current trace---encoded by the path condition---can be
extended with $c$ or $c$'s negation.  
This is done by formulating the constraints
as a satisfiability query; if the formula encoding branch constraints
is satisfiable, then there must exist an input that will follow the trace.
The trace is extended following the feasible branch outcomes, usually
in a depth-first manner.

In the example of Figure~\ref{fig:example}, on the leftmost path through
the control flow graph, when symbolic execution reaches the 
final branch it will record the condition ${\cal X} \le 60$, where
${\cal X}$ models the unknown value of input $x$.  This condition describes the
set of input values that will trigger execution of this path---as long
as both Bernoulli trials yield a value of 1.

In practice symbolic execution computes an underapproximation of 
program behavior.
Programs with looping behavior that is determined by input values 
may result in an infinite symbolic execution.
For this reason, symbolic execution is
typically run with a (user-specified) bound on the search depth, thus
some paths may be unexplored.   Moreover, there may be path constraints
for which efficient satisfiable checking is not possible.  Variants of
symbolic execution~\cite{godefroid2005dart,sen2005cute,song2008bitblaze},
called concolic execution, address this problem by replacing problematic 
constraints with equality constraints between variables 
and values collected while executing the program along the trace.

Symbolic execution tools and toolkits exist for many popular 
languages~\cite{pasareanu2010symbolic,godefroid2005dart,jamrozik2013generating,cadar2008klee}
and have been used primarily for test generation and fault detection.

\subsubsection{Probabilities and Probabilistic Models}

There is an enormous literature on probabilistic reasoning and statistics
that can be brought to bear in program analysis.  
In this paper, we consider two types of discrete time probabilistic models:
Markov chains and Markov decision processes (MDP)~\cite{Puterman:MDP94}.

Both models rely on the concept of a probability distribution.
A \textit{probability distribution} a probability distribution is a function 
that provides the probability of occurrence of different possible outcomes 
in an experiment.
%over a set is given by a function
%that maps each of the set's subsets to some real number between 0 and 1;
%this number represents the probability of a given subset being the outcome
%of a random experiment.
The sum of the probabilities for all outcomes is 1.

\ignore{
A \textit{distribution} over a set $S$ is given by a 
function $\pi : S \rightarrow [0,1]$ where
$\sum_{s \in S} \pi(s) = 1$.  For $s \in S$,
$\pi(s)$ is said to be the probability of $s$ -- it is
sometimes written Pr(s).
The set of distributions for $S$ is denoted $D_S$.

A Markov chain is a labeled transition system,
$(S,I,L,R)$, where $S$ is a set of states,
$I \subseteq S$ is a set of initial states (if there is a single
such state we denote it $s_0$), 
$L : S \rightarrow 2^{AP}$ is a function that assigns
sets of atomic propositions to a state, and
$R : S \rightarrow D_S$ defines for a given state
the probability of moving to another state.
The probability of executing a sequence of states $s = s_0, ..., s_n$, 
is $Pr(s) = \prod_{i \in [0,n-1]} R(i)(i+1)$.

Given a distinguished set of goal states, $G \subseteq S$,
we can define $Tr(G) = \{s_0, \ldots, s_n \vert s_0 \in I \wedge
s_n \in G \wedge \forall i \in [0,n-1] : R(s_i)(s_{i+1}) > 0$
as the traces reaching a $G$-state.
The probability of reaching a state in 
a set of goal states, $G$, is $Pr(G) = \sum_{s \in Tr(G)} Pr(s)$.
}

A Markov chain is a labeled transition system that, given some
state, defines the probability of moving to another state, according 
to a probability distribution.
The probability of executing a sequence of states is then the
product of the transition probabilities between the states.
The model fragment in the upper right corner of 
Figure~\ref{fig:example} depicts a Markov chain fragment.
It defines, for the set of states 
that are at the first line in the program, 
a 0.5 probability of transitioning to 
the state representing the beginning of the then block,
and similarly for the beginning of the else block.  The
distribution indicates a 0 probability of moving to any
other state.

For this small example, if we were to assume a probability
distribution on the input $x$, then it would be possible
to compute the probability of taking every edge in the CFG.   
This would be a Markov chain model of {\tt m} and it would replace
all nondeterministic choices in the CFG with probabilistic choices.

There are many situations where the removal of nondeterministic
choices is impractical or undesirable.  For example, if the input
distribution of $x$ is unknown, then retaining nondeterministic
choices for the conditionals which test that value would yield a
faithful program model.  In addition, it may be desirable, for
efficiency of analysis, to abstract program behavior, and that
abstraction may make it impossible to accurately compute the 
probability of a transition.  

Including nondeterminism in a probabilistic state
transition model yields a Markov decision process (MDP).
An MDP adds an additional structure, $A$, that defines
a set of (internal) actions which are used to model the
selection among a set of possible next-state probability distributions.
When traversing a path in an MDP, in each state, a choice
from $A$ must be made in order to determine how to transition,
probabilistically, to a next state.   That sequence of choices
\ignore{$\sigma \in A^*$,} is termed a \textit{policy} for the MDP.
Given a policy, an MDP reduces to a Markov chain.
One of the advantages of using MDP an lies in its ability to provide upper and
lower bounds on the behavior of the modeled system (discussed
in the following section).
\ignore{
For an MDP, let $Pr^{\sigma}(G)$ be the probability of reaching
a $G$-state from an initial state using schedule $\sigma$;
this is equivalent to $Pr(G)$ for the Markov chain induced
by fixing the schedule.  We can then define 
$Pr^+(G) = \max_{\sigma} Pr^{\sigma}(G)$
and
$Pr^-(G) = \min_{\sigma} Pr^{\sigma}(G)$
as the maximum and minimum probabilities of reaching a $G$-state
in the MDP over the set of all possible schedules.  
}
