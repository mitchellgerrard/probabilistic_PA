\section{Scope and Background}
\label{sec:background}

\input{example2}

We focus in this paper on programs that draw input variables
from given probability distributions, or equivalently that make
calls on functions returning values drawn from given distributions.
The left side of Figure~\ref{fig:example} shows a method,
\texttt{m}, that we will use to illustrate concepts in this paper.     
It takes an integer variable, \texttt{x}, as input, then based on the 
results of drawing values from a Bernoulli distribution
it either performs its computation
(which is unspecified and denoted with \texttt{...}) or triggers
an assertion. 

While researchers have developed analyses that consider a wide
range of program properties, here
we restrict our attention to program
properties that can be encoded as boolean predicates that
can be embedded in the program,
e.g., \texttt{assert} statements, to simplify the explanation
of how probabilities are incorporated into the analyses.
For the example, we are interested in reasoning about
\textit{assertion freedom}, i.e., that the program does
not reach a false assertion.
In our example
this can happen by reaching the implicit return from \texttt{m},
thereby by avoiding the three \texttt{assert} statements.

A static analysis capable of reasoning about assert statements 
can handle more general \textit{invariant properties}, i.e., 
predicates that must hold whenever reached in all program executions.  

\subsection{Programs and Program Analyses}
A program defines a set of execution \textit{traces}, each of
which is a sequence of \textit{concrete states}, i.e., 
the current program counter and a map from memory locations to values.
A program \textit{satisfies} an invariant property if, in all states of 
all traces, the predicate evaluates to true, otherwise the program
\textit{falsifies} the property.
In the example, there are six traces, half of which falsify the assertion
freedom property.

There are many different ways to represent the execution behavior
of a program to enable analysis.  Immediately to the right of
the code in Figure~\ref{fig:example} we show the control flow graph (CFG),
which explicitly represents control successor relationships between
statements.  A CFG models choice among successors as non-deterministic
choice -- depicted by the lack of labels on the edges.
We will also consider models that include probabilistic choice,
e.g., defining the probability that a branch is taken; the
upper right fragment of Figure~\ref{fig:example} shows probabilities
that reflect the outcome of the Bernoulli draw.
In addition, we will study models where 
choice of successor is defined based on the semantics
of program state, e.g., defining a condition under which the branch
is taken; the lower right fragment of Figure~\ref{fig:example}
shows logical conditions that 
reflect the fact that the value of parameter $x$ must be
less or equal to $60$ for control to traverse the branch.

A key concept in the program analysis frameworks we survey is
\textit{symbolic abstraction}.  A symbolic abstraction is a 
representation of a set of states.  Abstractions can be encoded
in a variety of forms, e.g., logical formula \cite{thakur2012bilateral}, binary
decision diagrams \cite{bryant1992symbolic}, or custom representations \cite{bagnara2008parma}.

Analyses that seek to prove the satisfaction of properties generally
define abstractions that \textit{overapproximate} the set of program
states, whereas those that seek to falsify properties generally define
abstractions that \textit{underapproximate} the set of program states.

With overapproximating analyses it is common to define an \textit{abstract
domain}, $\mathcal{A}$, 
which symbolically represents a set of concrete states which 
are said to be defined over the \textit{concrete domain}, $\mathcal{C}$.
For any reachable state of a program, a pair of abstraction 
and concretization functions, 
$\alpha : \mathcal{C} \mapsto \mathcal{A}$ and  
$\gamma : \mathcal{A} \mapsto 2^\mathcal{C}$,
serve to relate the concrete and abstract domains such that 
$\forall c \in \mathcal{C} : \alpha(c) \in \mathcal{A}$ and $c \in \gamma(\alpha(c))$.
Such an abstract domain is typically partially ordered, $\sqsupseteq$,
so that 
$\forall a,a' \in \mathcal{A} :  a \sqsupseteq a' \implies \gamma(a) \supseteq \gamma(a')$.
Moreover, equiping it with a meet operator, $\sqcap$, which
computes greatest lower bounds, and a maximal
value, $\top \in \mathcal{A}$ such that 
$\forall a \in \mathcal{A} : a \sqcap \top = a$, makes the
abstract domain a meet semi-lattice.

\subsubsection{Data Flow Analysis}
Data flow analyses~\cite{kildall1973unified} 
provides a framework for computing properties shared by sets of
program traces reaching a program state, or set of states.  It is
common for such analyses to group together the states that share a
common control location and the properties attempt to characterize
the invariants over those states.

Data flow analyses are built on an underlying fixpoint engine as is
model checking~\cite{Clarke:ModelCheckingBook}.  Moreover, data flow
analyses commonly operate on abstractions of program states that
can be defined by abstract interpretation~\cite{cousot1977abstract}. 
In fact it is now well-understood that data flow analysis can be
viewed as model checking of abstract interpretations~\cite{Schmidt:POPL}.

An abstract interpretation is a 
non-standard interpretation of program executions over an abstract domain.  
The semantics of program statements is lifted to operate
on a set of states, encoded as an element of the abstract domain,
rather than a single concrete state.  
For a program statement $\tau$,
$\tau^\#$ defines its abstract semantics such that
$\forall c, c' \in \mathcal{C} : \tau(c) = c' \implies \tau^\#(\alpha(c)) \sqsupseteq \alpha(c')$.  This implies the classic overapproximating correctness
relation for abstracted program statements:
$\tau^\# \sqsupseteq \alpha \circ \tau \circ \gamma$.

Conceptually for a program trace, $tr_i$, the sequence of 
statements, $[\tau_1,\ldots,\tau_n]$, defining the trace can
be interpreted functionally, $tr_i^\# = \tau_{n}^\# \circ \ldots \circ \tau_1^\#$,
and evaluated, $tr_i^\#(\iota)$,
to overapproximate the program states reached by the trace; 
here $\iota$ is the abstract state describing the initial program states.
Given an overapproximation of the set of traces, $T(l)$, leading to a 
program location, $l$, 
$\displaystyle\bigsqcap_j^{T(l)} tr_j^\#(\iota)$ 
defines invariant properties that hold prior to executing $l$; this
is refered to as the meet-over-paths (MOP) solution~\cite{kildall1973unified}.

Generating the set of traces for non-trivial programs is impractical 
and instead abstract states can be combined, via $\sqcap$, wherever
traces merge in the control flow and loops are processed
repeatedly to compute the maximum fix point (MFP)
as illustrated in Algorithm~\ref{alg-dfa}.
The algorithm stores abstract states, $a$, for each location, initializing
the first location to $\iota$, then proceeds along the control
flow relation of the program to approximate the effects of program
statements.  It removes a new location, $l$, and computes the abstract
state that approximates the post-state of executing the operation
at that location -- storing it in a temporary $t$.  Then for
all control flow successors it updates the abstract state for
that location by computing the greatest lower bound with $t$.
If that results in a new abstract state for the successor then
the successor is stored for future processing.  When the algorithm
terminates it computes for each location $l$ an approximation,
$a[l] \sqsubseteq \displaystyle\bigsqcap_j^{T(l)} tr_j^\#(\iota)$.

While not traditionally a component of a data flow analysis a property,
$\phi$, can be checked by relating it to abstract states.  Specifically,
if $a[l] \implies \phi$ then $\phi$ is verified to hold whenever 
location $l$ is reached.  However, it may be the case that $\phi$ holds
at some location $l'$ but $a[l'] \centernot\implies \phi$ -- this is refered
to as a \textit{false error} report and is due to the overapproximating
nature of $\mathcal{A}$ and $\tau^\#$.

Data flow analysis tools and toolkits exist for many popular 
languages~\cite{vallee1999soot,fink2012wala,others}
and have been used primarily for program optimization and
verifying program conformance with (implicit and explicit)
assertional specifications.

\ignore{
\subsubsection{Model Checking}
When applied to software, model checking techniques operate on 
labeled state transition
systems which encode the behavior of a program.
A transition system is defined as a tuple
$(S,I,L,R)$ where $S$ is a set of states,
$I \subseteq S$ is a set of initial states (if there is a single
such state we denote it $s_0$), 
$L : S \rightarrow 2^{AP}$ is a function that assigns
sets of atomic propositions to a state, and
$R : S \times S$ is the next state relation.

A transition system generates sequences of states, or traces.
Given a distinguished set of goal states, $G \subseteq S$,
we can define $Tr(G) = \{s_0, \ldots, s_n \vert s_0 \in I \wedge
s_n \in G \wedge \forall i \in [0,n-1] : (s_i,s_{i+1}) \in R$
as the traces reaching a $G$-state.
Determining the reachability of $G$-states is the goal of
model checking algorithms.

\mycomment{Matt: explain the algorithm}
}

\begin{figure}[t]
\noindent\begin{minipage}[t]{0.4\textwidth}
\begin{algorithm}[H]
%\renewcommand{\thealgorithm}{}
\renewcommand{\algorithmicindent}{0.6em}
\floatname{algorithm}{Alg.}
\caption{{\tt dfa}$(\iota,a)$}
\label{alg-dfa}
\begin{algorithmic}
 \STATE $a[l_0] \gets \iota$
 \STATE $\forall l \not= l_0 : a[l] \gets \top$
 \STATE $w \gets \{l_0\}$
 \WHILE{$w \not= \emptyset$}
   \STATE $l \gets remove(w)$
   \STATE $t \gets {op(l)}^{\#}(a[l])$
   \FOR{$l' \in succ(l)$}
     \STATE $a[l'] \gets (o \gets a[l']) \sqcap t$
     \IF{$a[l'] \not= o$}
       \STATE $w \gets w \cup \{l'\}$
     \ENDIF
   \ENDFOR
 \ENDWHILE
 \STATE $\forall l : \x{check}(a[l],\phi)$
\end{algorithmic}
\end{algorithm}
\end{minipage}%
\ignore{
\hfill
\begin{minipage}[t]{0.29\textwidth}
\begin{algorithm}[H]
%\renewcommand{\thealgorithm}{}
\renewcommand{\algorithmicindent}{0.6em}
\floatname{algorithm}{Alg.}
\caption{{\tt mc}$(s,seen)$}
\label{alg-mc}
\begin{algorithmic}
 \IF{$s \not\in seen$}
   \STATE $seen \gets seen \cup \{s\}$
   \STATE $\x{check}(L(s),\phi)$
   \FOR{$s' : R(s,s')$}
     \STATE {\tt mc}$(s',seen)$
   \ENDFOR
 \ENDIF
\end{algorithmic}
\end{algorithm}
\end{minipage}%
}
\hfill
\begin{minipage}[t]{0.4\textwidth}
\begin{algorithm}[H]
%\renewcommand{\thealgorithm}{}
\renewcommand{\algorithmicindent}{0.6em}
\floatname{algorithm}{Alg.}
\caption{{\tt symx}$(l,m,pc)$}
\label{alg-symexe}
\begin{algorithmic}
\IF{$\x{stoppingPath}(pc)$} 
 \RETURN
 \ENDIF
 \WHILE{$\neg \x{branch}(l)$}
   \STATE $m \gets \x{op}(l)(m)$
   \STATE $l \gets \x{succ}(l)$
   \STATE $\x{check}(l,m,\phi)$
 \ENDWHILE

 \STATE $c \gets \x{cond}(l)(m)$

 \IF{SAT$(pc \wedge c)$}
   \STATE {\tt symx}$(\x{succ_t}(l), m, pc \wedge c)$
 \ENDIF

 \IF{SAT$(pc \wedge \neg c)$}
   \STATE {\tt symx}$(\x{succ_f}(l), m, pc \wedge \neg c)$
 \ENDIF
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure}

\subsubsection{Symbolic Execution}
Like data flow analysis, symbolic execution~\cite{king1976symbolic,clarke1976system} 
performs a non-standard interpretation of program executions using 
a symbolic abstraction of program states.
Algorithm~\ref{alg-symexe} sketches the symbolic execution algorithm.
The algorithm records the current program location, $l$,
and a map, $m$, that records symbolic expressions encoding the
values of program memory.  In addition, a \textit{path condition}, $pc$,
accumulates symbolic expressions that encode branch constraints 
taken along a trace.  The analysis is initiated with the first
program location, $l_0$, an empty map, and $pc = true$.

Sequences of program statements are interpreted by applying the operation
at each program location, $op(l)$, to update the map.  
An operation that reads from an input generates a fresh symbolic
variable which is unconstrained.  
When a branching statement is encountered, the symbolic expression encoding
the branch condition is computed, $c$, and a check is performed
to determine whether the current trace, encoded by $pc$, can be
extended with $c$ or its negation.  
In the example of Figure~\ref{fig:example}, on the leftmost trace through
the control flow graph, symbolic execution would use the semantic
condition shown in the lower right, $c \equiv {\cal X} \le 60$,
and add it to the path condition -- which is initially $true$.
This is achieved by formulating the constraints
as a satisfiability query -- if the formula encoding branch constraints
is satisfiable then there must exist an input that will follow the trace.
For the example, all of the 6 traces through the control flow graph 
result in satisfiable path conditions.
The trace is extended, recursively, following the feasible branch outcomes
in a depth-first manner.

While this algorithm is capable of computing an \textit{exact} symbolic
approximation of the set of program states on a trace reaching $l$, in
practice symbolic execution computes an underapproximation.
Programs with looping behavior that is determined by input values 
may result in an infinite symbolic execution tree. 
For this reason, symbolic execution is
typically run with a (user-specified) bound on the search depth, thus
some paths may be unexplored (checked in {\tt stoppingPath}).   Moreover, there may be path constraints
for which efficient satisfiable checking is not possible.  Variants of
symbolic execution~\cite{godefroid2005dart,sen2005cute,song2008bitblaze} 
address this by replacing problematic $pc$ constraints with equality
constraints based on values collected by executing the program along the trace.

It is straightforward to integrate the checking of a property,
$\phi$, into this algorithm.  Specifically,
if $m \implies \neg\phi$ then $\phi$ is falsified 
when location $l$ is reached.    This does not mean that $\phi$
fails to hold whenever $l$ is reached, but due to the underapproximation
of symbolic execution there is guaranteed to be an execution for which
it fails to hold.
However, it may be the case that $\phi$ fails to hold
on some trace reaching location $l$, but this may be missed
due to the underapproximating nature of symbolic execution.
In other words, if symbolic execution fails to find an error
then one cannot conclude the lack of errror.

Symbolic execution tools and toolkits exist for many popular 
languages~\cite{pasareanu2010symbolic,godefroid2005dart,jamrozik2013generating,cadar2008klee}
and have been used primarily for test generation and fault detection.

\subsection{Probabilities and Probabilistic Models}

There is an enormous literature on probability and statistics
that can be brought to bear in program analysis.  
In this paper, we consider two types of discrete time models:
Markov chains and Markov decision processes (MDP).

A \textit{distribution} over a set $S$ is given by a 
function $\pi : S \rightarrow [0,1]$ where
$\sum_{s \in S} \pi(s) = 1$.  For $s \in S$,
$\pi(s)$ is said to be the probability of $s$ -- it is
sometimes written Pr(s).
The set of distributions for $S$ is denoted $D_S$.

A Markov chain is a labeled transition system,
$(S,I,L,R)$, where $S$ is a set of states,
$I \subseteq S$ is a set of initial states (if there is a single
such state we denote it $s_0$), 
$L : S \rightarrow 2^{AP}$ is a function that assigns
sets of atomic propositions to a state, and
$R : S \rightarrow D_S$ defines for a given state
the probability of moving to another state.
The probability of executing a sequence of states $s = s_0, ..., s_n$, 
is $Pr(s) = \prod_{i \in [0,n-1]} R(i)(i+1)$.

Given a distinguished set of goal states, $G \subseteq S$,
we can define $Tr(G) = \{s_0, \ldots, s_n \vert s_0 \in I \wedge
s_n \in G \wedge \forall i \in [0,n-1] : R(s_i)(s_{i+1}) > 0$
as the traces reaching a $G$-state.
The probability of reaching a state in 
a set of goal states, $G$, is $Pr(G) = \sum_{s \in Tr(G)} Pr(s)$.

The model fragment in the upper right corner of 
Figure~\ref{fig:example} depicts a Markov chain fragment.
It defines, for the set of states 
that are at the first line in the program, 
a 0.5 probability that they will transition to 
the state representing the beginning of the then block,
and similarly for the beginning of the else block.  The
distribution indicates a 0 probability of moving to any
other state.

For this small example, if we were to assume a probability
distribution on the input $x$, then it would be possible
to compute the probability of taking every edge in the CFG.   
This would be a Markov chain model of $m$ and it would replace
all non-deterministic choice in the CFG with probabilistic choice.

There are many situations where the removal of non-deterministic
choice is impractical or undesirable.  For example, if the input
distribution of $x$ is unknown then retaining non-deterministic
choices for the conditionals testing that value would yield a
faithful program model.  In addition, it may be desirable, for
efficiency of analysis, to abstract program behavior, and that
abstraction may make it impossible to accurately compute the 
probability of a transition.  An example of this is modeling
the scheduler policy in reasoning about the behavior of a multi-threaded
program.

When non-determinism is included in a probabilistic state
transition model it results in an MDP.  
An MDP adds an additional structure $A$ that defines
a set of (internal) actions which are used to model the
selection among a set of possible next-state probability distributions.
The transition relation
for MDPs is generalized to $R : S \times A \times D_S$.
When traversing a path in an MDP, in each state a choice
from $A$ must be made in order to determine how to transition,
probabilistically, to a next state.   That sequence of choices, 
$\sigma \in A^*$, is termed a \textit{schedule} (or policy or 
strategy) for the MDP.

Given a fixed schedule an MDP reduces to a Markov chain, but
the value in an MDP lies in its ability to provide upper and
lower bounds on the behavior of the modeled system.
For an MDP, let $Pr^{\sigma}(G)$ be the probability of reaching
a $G$-state from an initial state using schedule $\sigma$;
this is equivalent to $Pr(G)$ for the Markov chain induced
by fixing the schedule.  We can then define 
$Pr^+(G) = \max_{\sigma} Pr^{\sigma}(G)$
and
$Pr^-(G) = \min_{\sigma} Pr^{\sigma}(G)$
as the maximum and minimum probabilities of reaching a $G$-state
in the MDP over the set of all possible schedules.  
